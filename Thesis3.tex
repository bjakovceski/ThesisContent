% arara: xelatex
% arara: xelatex
% arara: xelatex

% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=M,english]{FITthesis}[2018/05/30]

\usepackage{graphicx} %graphics files inclusion
% \usepackage{subfig} %subfigures
\usepackage{amsmath} %advanced maths
\usepackage{amssymb} %additional math symbols
\usepackage[]{algorithm2e} %for algorithms

\usepackage{dirtree} %directory tree visualisation

\usepackage{listings}
%\usepackage{booktabs, makecell, longtable}

\usepackage[section]{placeins}
\usepackage{float}
\graphicspath{ {"C:/Users/Jakovcheski/Desktop/Stanford NER output/OneAnnotationAbstractBaseOnPageRank/"} }

% list of acronyms
 \usepackage[acronym,nonumberlist,toc,numberedsection=autolabel,nomain]{glossaries}
\iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
\makeglossaries

%\newcommand{\tg}{\mathop{\mathrm{tg}}} %cesky tangens
%\newcommand{\cotg}{\mathop{\mathrm{cotg}}} %cesky cotangens


\setcounter{tocdepth}{2} %define depth of showing in table of content
\setcounter{secnumdepth}{2} %define depth of showing in table of content


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\department{Department of software engineering}
\title{Domain-specific Named Entity Recognition}
\authorGN{Bogoljub} %author's given name/names
\authorFN{Jakovcheski} %author's surname
\authorWithDegrees{Bc. Bogoljub Jakovcheski} %author's name with academic degrees
\author{Bogoljub Jakovcheski} %author's name without academic degrees
\supervisor{Ing. Milan Doj{\v c}inovski}
\acknowledgements{I would like to thank my family and friends for support during writing this thesis.}
\abstractCS{}
\abstractEN{}
\placeForDeclarationOfAuthenticity{Prague} %where you have signed the declaration
\keywordsCS{}
\keywordsEN{}
\declarationOfAuthenticityOption{4} %select as appropriate, according to the desired license

\begin{document}
% \newacronym{LZW}{LZW}{Lempel Ziv Welch}
% \newacronym{RLE}{RLE}{Run-Length Encoding}

\begin{introduction}

\section{Motivation}
    Named Entity Recognition (NER)\cite{wiki:NER} is locating and classifying named entities in text into some pre-defined categories such as locations, organizations, person name, sport etc. Today NER is used to different areas from full-text search and filtering to preprocessing tool for other NLP tasks \cite{master:NER}.  	
		
    Most NER applications are trained on a general text and on a specific domain, the problem is that they are optimized for the specific type of data i.e. specific domain. That means that those NER applications can give nice results on texts or domains that are trained, but bad results for texts on a specific domain for which that NER is not trained.
	  
    Most of the NER applications are trained on a small number of types. For example, at the moment of writing this thesis, Stanford NER\footnote{http://nlp.stanford.edu:8080/ner/} has a model that have maximum 7 types, Dbpedia Spotlight\footnote{https://www.dbpedia-spotlight.org/demo/} has model with 31 types, spaCy\footnote{https://spacy.io/usage/linguistic-features} build-in model has 18 types and spaCy Wikipedia scheme model have 4 types.
	  
	The main goal of this thesis is to research possibilities of training NER models for a specific domain. To achieve this goal it is necessary to create datasets for certain domains. This research is focused on 3 domains, "POLITICS", "SPORT" and "TRANSPORTATION". Every domain is created with a certain number on types from DBpedia Ontology, then for creating datasets is used DBpedia NIF who gives an opportunity to approaches to information from Wikipedia abstracts, for example, types that annotated words has in those abstracts.
	  
	  Thesis research which is the quality of trained domains, the impact of the size of the data and the quality of the defined domains.
	  
	  
\section{Goals of the thesis}	
	Nevertheless, vast majority of the developed NER systems have been developed as general-purpose systems. While they can perform well on multiple domains (macro level), on specific domains (micro level) their performance quality might be low. The ultimate goal of the thesis is to develop domain-specific NER models. Guidelines:
\begin{itemize}
	\item Investigate possible datasets for domain-specific training of NER.
	\item Develop NER training datasets for several selected domains (e.g. sports,
politics, music, etc.).
	\item Train a domain-specific NER model using existing frameworks, such as
DBpedia Spotlight or StanfordNER.
	\item Validate and evaluate the developed domain-specific NER models. 
\end{itemize}	
	
\section{Thesis outline}
	

\end{introduction}

\chapter{Background and related work}\label{}

	%This thesis was submitted at Czech Technical University in Prague (see Figure~\ref{fig:logo}).
	
	%\begin{figure}[H]\centering
		%\includegraphics{cvut-logo-bw}
		%\caption{CTU logo}\label{fig:logo}
	%\end{figure}
\section{Background}

\subsection{Information extraction}
	Information extraction first appears in late 1970s within NLP field\footnote{https://www.slideshare.net/rubenizquierdobevia/information-extraction-45392844 slide 4 of 69}. 
Information extraction (IE) \cite{wiki:IE} is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases, this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.

	Another view of that what Information extraction is that automatically building a relational database from information contained in unstructured text. Unlike linear-chain models, general CRFs can capture long distance dependencies between labels \cite{article:IE}.

	To understand better what IE is let's give trivial example\footnote{https://ontotext.com/knowledgehub/fundamentals/information-extraction/}. Imagine receiving an email message with some date in it. So extracting information from mail message and adding to your Calendar is part of IE. Millions of people use this on their daily basis and they are not aware of that how that works and what technology is used for that.

	Figure~\ref{fig:InformationExtraction} gives us a closer look at what Information extraction (IE) is, and how State-of-the-Art algorithms transform unstructured text to structured sequences understandable for machines. 

	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{information-extraction}
		\caption[Information extraction example]{Information extraction, downloaded from\footnotemark}
	\end{figure}
\footnotetext{\url{https://www.slideshare.net/rubenizquierdobevia/information-extraction-45392844}}
%https://web.stanford.edu/~jurafsky/slp3/21.pdf


\subsection{Named Entity Recognition}\label{NER subsection}
Named Entity Recognition (NER) \cite{article:NER} is the problem of identifying and classifying proper names in text, including locations, such as China; people, such as George Bush; and organizations, such as the United Nations. The named-entity recognition task is, given a sentence, first to segment which words are part of entities, and then to classify each entity by type (person, organization, location, and so on). The challenge of this problem is that many named entities are too rare to appear even in a large training set, and therefore the system must identify them based only on context.

One approach to NER is to classify each word independently as one of either Person, Location, Organization, or Other (meaning not an entity). The problem with this approach is that it assumes that given the input, all of the named entity labels are independent. In fact, the named-entity labels of neighboring words are dependent; for example, while New York is a location, New York Times is an organization.

%The first step in most IE tasks is to find the proper names or named entities mentioned in a text. The task of named entity recognition (NER) is to find each mention of a named entity in the text and label its type. What constitutes a named entity type is application specific; these commonly include people, places, and organizations but also more specific entities from the names of genes and proteins(Cohen and Demner-Fushman, 2014) to the names of college courses (McCallum,2005).%[https://web.stanford.edu/~jurafsky/slp3/21.pdf]

%Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction (IE) that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. \cite{wiki:NER}

Most research on NER systems has been structured as taking an unannotated block of text, such as this one:

Jim bought 300 shares of Acme Corp. in 2006.

And producing an annotated block of text that highlights the names of entities:

[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.

In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified \cite{wiki:NER}.

	Figure~\ref{fig:StanfordNER} shows how one NER application can look like. The text in the example is predefined in Stanford NER application and loaded model (Classifier) is also trained by Stanford\footnote{https://nlp.stanford.edu/software/CRF-NER.html\#Models}.

	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{NER-Stanford}
		\caption{Stanford NER GUI with 3 classes model (Location, Person, Organization)}\label{fig:StanfordNER}
	\end{figure}

	There are several applications or frameworks for NER like Stanford NER, DBpedia Spotlight, spaCy, Chatbot NER, GATE, OpenNLP and so on. Here we will take a look only on the mentioned ones.

\subsubsection{Stanford NER}\label{Stanford NER}
Stanford NER\footnote{https://nlp.stanford.edu/software/CRF-NER.html} is a Java implementation of a Named Entity Recognizer. Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. It comes with well-engineered feature extractors for Named Entity Recognition, and many options for defining feature extractors. Included with the download are good named entity recognizers for English, particularly for the 3 classes (PERSON, ORGANIZATION, LOCATION), and we also make available on this page various other models for different languages and circumstances, including models trained on just the CoNLL 2003 English training data.

Stanford NER is also known as CRFClassifier. The software provides a general implementation of (arbitrary order) linear chain Conditional Random Field (CRF) sequence models. That is, by training your own models on labeled data, you can actually use this code to build sequence models for NER or any other task \cite{article:StanfordNER}.

\subsubsection{DBpedia Spotlight}\label{DBpedia Spotlight}
DBpedia Spotlight\footnote{https://www.dbpedia-spotlight.org/} \cite{wiki:DBpediaSpotlight} is a tool for annotating mentions of DBpedia resources in text. This allows linking unstructured information sources to the Linked Open Data cloud through DBpedia. DBpedia Spotlight performs named entity extraction, including entity detection and name resolution (in other words, disambiguation). It can also be used for named entity recognition, and other information extraction tasks. DBpedia Spotlight aims to be customizable for many use cases. Instead of focusing on a few entity types, the project strives to support the annotation of all 3.5 million entities and concepts from more than 320 classes in DBpedia. The project started in June 2010 at the Web Based Systems Group at the Free University of Berlin.

\subsubsection{spaCy}\label{spaCy}
spaCy\footnote{https://spacy.io/} \cite{wiki:spaCy} is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython. It offers the fastest syntactic parser in the world. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages.

\subsubsection{GATE}\label{GATE}
General Architecture for Text Engineering or GATE\footnote{https://gate.ac.uk/} \cite{wiki:GATE} is a Java suite of tools originally developed at the University of Sheffield beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many natural language processing tasks, including information extraction in many languages.

GATE includes an information extraction system called ANNIE (A Nearly-New Information Extraction System)\footnote{http://services.gate.ac.uk/annie/} which is a set of modules comprising a tokenizer, a gazetteer, a sentence splitter, a part of speech tagger, a named entities transducer and a coreference tagger. ANNIE can be used as-is to provide basic information extraction functionality, or provide a starting point for more specific tasks.

\subsubsection{OpenNLP}\label{OpenNLP}
The Apache OpenNLP library\footnote{http://opennlp.apache.org/docs/1.8.4/manual/opennlp.html\#intro.description} is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. OpenNLP also included maximum entropy and perceptron based machine learning.

The goal of the OpenNLP project will be to create a mature toolkit for the abovementioned tasks. An additional goal is to provide a large number of pre-built models for a variety of languages, as well as the annotated text resources that those models are derived from.

\subsubsection{Chatbot NER}\label{Chatbot NER}
Chatbot NER\footnote{https://haptik.ai/tech/open-sourcing-chatbot-ner/} is heuristic based that uses several NLP techniques to extract necessary entities from chat interface. In Chatbot, there are several entities that need to be identified and each entity has to be distinguished based on its type as a different entity has different detection logic. 



\subsection{RDF/NIF}
	The Resource Description Framework (RDF)\cite{wiki:RDF} is a family of World Wide Web Consortium (W3C) specifications originally designed as a metadata data model.  It is a framework for describing resources on the web; it is designed to be read and understood by computers.
	
	The information in RDF is represented by subject-predicate-object, known as triples. Triples are written in one of RDF notations: RDF/XML, RDFa, N-Triples, Turtle, JSON-LD and stored in a triplestore \cite{master:SPARQL}.
	
	RDF \cite{w3c:RDF} has features that facilitate data merging even if the underlying schemas differ, and it specifically supports the evolution of schemas over time without requiring all the data consumers to be changed.

	Natural Language Processing Interchange Format (NIF)\footnote{http://aksw.org/Projects/NIF.html} \cite{w3c:NIF} is an RDF-based format. The classes to represent linguistic data are defined in the NIF Core Ontology. All ontology classes are derived from the main class nif:String which respresents strings of Unicode characters.
	
%	\begin{lstlisting}[language=java]
%https://2018.eswc-conferences.org/wp-content/uploads/2018/02/ESWC2018_paper_136.pdf
%1 @ prefix rdf : <http :// www .w3.org /1999/02/22 - rdf - syntax -ns#> .
%2 @ prefix xsd : <http :// www .w3.org /2001/ XMLSchema #> .
%3 @ prefix itsrdf : <http :// www .w3. org /2005/11/ its / rdf #> .
%4 @ prefix nif : <> .
%5 @ prefix ex: <http :// nif . dbpedia . org/ wiki /en/> .
%6
%7 ex : United_States ? dbpv =2016 -10& nif = context
%8 a nif : Context ;
%9 nif : beginIndex "0"^^ xsd : nonNegativeInteger ;
%10 nif : endIndex " 104211 "^^ xsd : nonNegativeInteger ;
%11 nif : firstSection ex : United_States ? dbpv =2016 -10& char =0 ,4241 ;
%12 nif : lastSection ex : United_States ? dbpv =2016 -10& char =103211 ,104211 ;
%13 nif : hasSection ex : World_War_II ? dbpv =2016 -10& char =0 ,5001 ;
%14 nif : sourceUrl ex : United_States ? oldid =745182619 ;
%15 nif : predLang < http :// lexvo . org / id / iso639 -3/ eng > ;
%16 nif : isString " ... The first inhabitants of North America migrated from
%Siberia by way of the Bering land bridge ... " .
%17
%18 ex : United_States ? dbpv =2016 -10& char =7745 ,9418
%19 a nif : Section ;
%20 nif : beginIndex " 7745 "^^ xsd : nonNegativeInteger ;
%21 nif : endIndex " 9418 "^^ xsd : nonNegativeInteger ;
%22 nif : hasParagraph ex : United_States ? dbpv =2016 -10& char =7860 ,8740 ;
%23 nif : lastParagraph ex : United_States ? dbpv =2016 -10& char =8741 ,9418 ;
%24 nif : nextSection ex : United_States ? dbpv =2016 -10& char =9420 ,12898 ;
%25 nif : referenceContext ex : United_States ? dbpv =2016 -10& nif = context ;
%26 nif : superString ex : United_States ? dbpv =2016 -10& char =7548 ,7743 .
%27
%28 ex : United_States ? dbpv =2016 -10& nif = paragraph & char =7860 ,8740
%29 a nif : Paragraph ;
%30 nif : beginIndex " 7860 "^^ xsd : nonNegativeInteger ;
%31 nif : endIndex " 8740 "^^ xsd : nonNegativeInteger ;
%32 nif : nextParagraph ex : United_States ? dbpv =2016 -10& char =8741 ,9418 ;
%33 nif : referenceContext ex : United_States ? dbpv =2016 -10& nif = context ;
%34 nif : superString ex : United_States ? dbpv =2016 -10& char =7745 ,9418 .
%35
%36 ex : United_States ? dbpv =2016 -10& char =7913 ,7920
%37 a nif: Word ;
%38 nif : anchorOf " Siberia " ;
%39 nif : beginIndex " 7913 "^^ xsd : nonNegativeInteger ;
%40 nif : endIndex " 7920 "^^ xsd : nonNegativeInteger ;
%41 nif : referenceContext ex : United_States ? dbpv =2016 -10& nif = context ;
%42 nif : superString ex : United_States ? dbpv =2016 -10& char =7860 ,8740 ;
%43 itsrdf : taIdentRef < http :// dbpedia . org / resource / Siberia > .
%	\end{lstlisting}

\subsection{DBpedia}
DBpedia \cite{dbpedia:core} is a crowd-sourced community effort to extract structured content from the information created in various Wikimedia projects. This structured information resembles an open knowledge graph (OKG) which is available for everyone on the Web. A knowledge graph is a special kind of database which stores knowledge in a machine-readable form and provides a means for information to be collected, organised, shared, searched and utilised. Google uses a similar approach to create those knowledge cards during search.

	DBpedia data is served as Linked Data, which is revolutionizing the way applications interact with the Web. One can navigate this Web of facts with standard Web browsers, automated crawlers or pose complex queries with SQL-like query languages (e.g. SPARQL).

	At the time of writing this thesis the last version of DBpedia is 3.7.
\subsubsection{DBpedia NIF}\label{DBpediaNIF}
	DBpedia \cite{dbpedia:NIF} currently primarily focus on representing factual knowledge as contained in Wikipedia infoboxes. A vast amount of information, however, is contained in the unstructured Wikipedia article texts. In order to broaden and deepen the amount of structured DBpedia data, we are going a step further.

	With the representation of wiki pages in the NLP Interchange Format (NIF) we provide all information directly extractable from the HTML source code divided into three datasets:
	\begin{itemize}
		\item nif-context: the full text of a page as context (including begin and end index)
		\item nif-page-structure: the structure of the page in sections and paragraphs (titles, subsections etc.)
		\item nif-text-links: all in-text links to other DBpedia resources as well as external references
	\end{itemize}
	These datasets will serve as the groundwork for further NLP fact extraction tasks to enrich the gathered knowledge of DBpedia.

For the purposes of this thesis we will use DBpedia NIF dataset version 2016-04 (dbpv=2016-04).

\subsubsection{DBpedia ontology}\label{DBpediaOntology}
	The DBpedia Ontology is a shallow, cross-domain ontology, which has been manually created based on the most commonly used infoboxes within Wikipedia. The ontology currently covers 685 classes which form a subsumption hierarchy and are described by 2,795 different properties.

	Since the DBpedia 3.7 release, the ontology is a directed-acyclic graph, not a tree. Classes may have multiple superclasses, which was important for the mappings to schema.org. A taxonomy can still be constructed by ignoring all superclasses except the one that is specified first in the list and is considered the most important \cite{dbpedia:Ontology}. 

Dbpedia ontology classes can be found here \footnote{\url{http://mappings.dbpedia.org/server/ontology/classes/}}

The DBpedia Ontology currently contains about 4,233,000 instances. Figure~\ref{fig:Dbpedia-ontology} shows the number of instances for several classes within the ontology. [http://wiki.dbpedia.org/services-resources/ontology]

	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{Dbpedia-ontology}
		\caption{Dbpedia Ontology - Instances per class}\label{fig:Dbpedia-ontology}
	\end{figure}

\subsection{Apache Jena}\label{ApacheJena}
	Apache Jena\footnote{\url{https://jena.apache.org/index.html}} \cite{wiki:ApacheJena} is an open source Semantic Web framework for Java. It provides an API to extract data from and write to RDF graphs. The graphs are represented as an abstract "model". A model can be sourced with data from files, databases, URLs or a combination of these. A Model can also be queried through SPARQL 1.1.

\subsection{SPARQL}\label{SPARQL}
	SPARQL \cite{master:SPARQL} is an RDF query language, that is, a semantic query language for databases, able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. SPARQL works for any data source that can be mapped to RDF.

	SPARQL allows users to write queries against key-value data or, more specifically, data that can be mapped to RDF. The entire database is thus a set of subject-predicate-object triples.
	
	The SPARQL standard \footnote{\url{https://ontotext.com/knowledgehub/fundamentals/what-is-sparql/}} is designed and endorsed by the W3C and helps users and developers focus on what they would like to know instead of how a database is organized.

	In Listing~\ref{SPARQLexample} is an example of SPARQL query where we are selecting 10 abstracts from DBpedia NIF who has ontology type PoliticalParty and their PageRank and sort descending by PageRank.   
\begin{lstlisting}[caption=SPARQL example \label{SPARQLexample}]
PREFIX rdf:<http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX dbo:<http://dbpedia.org/ontology/>
PREFIX vrank:<http://purl.org/voc/vrank#>

SELECT DISTINCT ?s ?v
FROM <http://dbpedia.org>
FROM <http://people.aifb.kit.edu/ath/#DBpedia_PageRank>
WHERE{
	?s rdf:type dbo:PoliticalParty .
	?s vrank:hasRank/vrank:rankValue ?v.
}
ORDER BY DESC(?v) LIMIT 10
\end{lstlisting}

\section{Related work}
In this section we will compare our approach and our chosen domains.

\subsection{Domain specific Named Entity Recognition}
Traditionally Named Entity Recognition (NER)\cite{article:DomainSpecific} systems have been built using available annotated datasets (like CoNLL, MUC) and demonstrate excellent performance. However, these models fail to generalize onto other domains like Sports and Finance where conventions and language use can differ significantly. Furthermore, several domains do not have large amounts of annotated labeled data for training robust Named Entity Recognition models.
With specifying the domain we can create a bigger model with more annotated words and reading the whole text will be same or even faster that reading text with a global domain.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%NEW CHAPTER%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%CHAPTER 2%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Domain specific named entity recognition}\label{}
	In this chapter we will go through the whole process of transforming raw DBpedia datasets to datasets that are ready for training a model with Stanford NER and how to train a model with Stanford NER. Section~\ref{dataPreProcessing} explains the process of cleaning the data from DBpedia NIF datasets and preparing them for processing. In Section~\ref{domainSpecification} we explain how we choose "POLITICS", "SPORT" and "TRANSPORTATION" domains. Section~\ref{typesRetrieval} shows all ontology types that we retrieve for every domain and grouping them to more specific ontology type. In Section~\ref{StanfordNERoutput} is explained the process of preparing datasets for training in Stanford NER. And finally in Section~\ref{modelGeneration} is shown how to train a datasets with Stanford NER.  

\section{Data pre-processing}\label{dataPreProcessing}
	To be able to create domain specific datasets ideally we need some big raw data. We choose data from DBpedia NIF Datasets (for more information about DBpedia NIF see Section~\ref{DBpediaNIF}) for the English language in .ttl format. From here we needed only 2 datasets, and that nif-context (or nif-abstract-context) and nif-text-links.

	Another dataset that we needed was DBpedia instance types dataset, found at DBpedia download page\footnote{\url{http://wiki.dbpedia.org/downloads-2016-04}} also in .ttl format. This dataset contains all types of nif-text-links that occurrence at nif-abstract-context file.

	So how all this dataset are connected between themselves? Let say that we have abstract for Alexander the Great. In nif-text-links file we have all words from the abstract that has annotation, but we still don't know their type. So here comes instance types file where based on link from nif-text-link (eg.http://dbpedia.org/resource/Philip\_II\_of\_Macedon) we can find the type of annotated word (word Philip II has ontology type Monarch), but of course, there can be a case that some words cannot be found on instance types file and automatically have no type, or in our case ontology type O (O stands for OTHER).

	But now, let us explain deeply how we process and clean data from the datasets. First, we define small test dataset to check how fast we can process data. Running that dataset on downloaded files without any cleaning on data takes too long. So we said that converting the datasets from RDF format to binary format (.ttl to .hdt) with RDF/HDT tool\footnote{\url{http://www.rdfhdt.org/}} will be faster. HDT (Header, Dictionary, Triples)\cite{FMPGPA:13} is a compact data structure and binary serialization format for RDF that keeps big datasets compressed to save space while maintaining search and browse operations without prior decompression. So we converted the datasets and reran the algorithm again. There were some improvements, but not satisfying for our purposes. Our next solution was to clean datasets from unused data for our aims. The final result after cleaning was a smaller datasets, for instance, nif-abstract-context file from 7.78GB now has 2.99GB, another big improvement was nif-text-links file who is reduced to 10.5GB from 44.6GB and at the end we also clean instance-types file, but here we don't record any mayor memory improvements. Again we rerun the algorithm, of course, there were improvements, but as well as previous the time that algorithm runs, was not acceptable for us. To give an illustration, the time needed to find all types from one abstract in a worst case, to read nif-text-links and instance type files until the end was around 3.5 minutes. Therefore once more we converted our cleaned datasets from RDF format(.ttl) to binary format(.hdt). And how in previous running there were again improvements, but those improvements don't fulfill our expectations. The final thing that we have to save us was creating a dataset tree only for nif-text-links and instance-types files. For nif-text-links file we created a tree where we have folders from "a-z", also special characters folders and other folder(this folder contains data that have a lower occurrence, let say \& character or letters that are not part of the English alphabet) and folders from "a-z" has subfolders also from "a-z". 

	To give a closer look how we create that tree, let say that we have an abstract for Volkswagen Golf MK3, so the link for that abstract would be http://dbpedia.org/resource/Volkswagen\_Golf\_Mk3 and this link will be stored to "v" folder and "o" subfolder, because the title of the abstract is Volkswagen Golf MK3, where we need only first 2 letters from the first word, in this case word Volkswagen. With this, we have a smaller dataset where we can read the whole one very fast.

	For instance types file we modified the algorithm for creating a data tree. Here because of lower range data we have created only files from "a-z", of course, special characters files and other file.

	Finally, we rerun the algorithm, and the time to process one abstract, at worst case, takes no longer than 1 minute. Now we were ready to take next steps to retrieve types (see Section~\ref{typesRetrieval}), create domains (see Section~\ref{domainSpecification}) and prepare data for Stanford NER (see Section~\ref{StanfordNERoutput}).

\section{Domain specification}\label{domainSpecification}
	As we said earlier most of the NER application are trained on same domains, like "PERSON", "ORGANIZATION" and "LOCATION". These 3 domains are widely spread all over the applications and perform nice results on text from this domains. So what we need is something that is not already trained or there is a small usage of that domain. After some research, we find out that "TRANSPORTATION" domain is not a popular domain for NER applications, respectively in time of writing the thesis we don't find any usage of this specific domain. So there is the possibility to create this specific domain. Types that we retrieve for this domain and groping them to more specific types are more deeply explained in Types retrieval (see Section~\ref{typesRetrieval}). We have our first domain, but at least 2 more domains are needed to be able to make some experiments and conclusion. 

	Ideally will be those domains to have some connection between them and again not to be already widespread. So we look at ontology types that are retrieved for "TRANSPORTATION" domain, and there are types like Airport, Bridge, MetroStation and so on. This indicates to us that next domain can be "POLITICS". Why? Because some airports, bridges or metro stations bear names of Politicians. For instance airport in Prague, Czech Republic is named by the last president of Czechoslovakia, Vaclav Havel. Or another example is that some bridges in the United States are named by famous politicians, like Presidents. The types that contains this domain are explained in Section~\ref{typesRetrieval}. The second domain is chosen, so we need at least one more domain to keep up with other NER applications. 
	
	With the requirements that we set for choosing a domain, which domain to choose was not easy at all. After a big research, also referring to ontology types from previous two domains and some NER applications (see Section~\ref{NER subsection}) we find an opportunity to create the last "SPORT" domain. Now we should check on DBpedia ontology classes page (see Section~\ref{DBpediaOntology}) how many ontology types we have for this domain. At the time of writing this thesis there were around 170 ontology types, which is very good number for creating a domain (for more see Section~\ref{typesRetrieval}).
	
After we complete choosing of domains, the next big think was to choose the right ontology types for every specific domain and if it is needed or make sense group those types to more specific type. This is totally covered and explained at Section~\ref{typesRetrieval}. 

\section{Types retrieval}\label{typesRetrieval}
	After we solved the problem of that how effectively run the algorithm to find all types from the abstract and choose domains, next issue was which types we want to be part of our domains and also which types we want to retrieve from Dbpedia. Worth mentioning that we will use the same ontology types for retrieving the abstracts links from Dbpedia and creating a domain models. For example the type "Politician" will be used to retrieve links from Dbpedia that has that type, and also "Politician" type will be use to annotated words, for instance Barack Obama will have type of "Politician" (we will give more details on section~\ref{StanfordNERoutput}).

	In DBpedia ontology classes page\footnote{\url{http://mappings.dbpedia.org/server/ontology/classes/}} we can see all types that DBpedia ontology has. Those ontology types are the same in instance types file also. Now we are facing with the fact that if we choose very small group of ontology types, at the experiment point we will have minor range of annotated words and experiments won't be relevant. On the other hand, if we go too deep to ontology types, we will have a lot of annotated words, which on one hand is good, but training the model will take a lot of time and memory, and there is a possibility that we will reach memory exception, or because of big group of types training will never end.  

After some testing with the number of retrieved types we finally found the best selection of types, in total we choose 283 ontology types for all domains.

Now let us explain more deeply every single domain and which types has that domain. We have 3 domains (see Section~\ref{domainSpecification} for that how we choose those domains) "POLITICS", "SPORT" and "TRANSPORTATION".

In "POLITICS" domain we retrieve in total 26 types, found at Appendix~\ref{PoliticsTypes}, which we sort in 11 more specific types like Ambassador, Chancellor, Congressman, Deputy, Governor, Lieutenant, Mayor, MemberOfParliament, Minister, President, PrimeMinister, Senator, VicePresident and VicePrimeMinister are joined together in one specific domain Politician, other types we leaved as it is, because if we group them the types wouldn't give any sense.  

	We do the same for "SPORT" domain where we retrieve in total 171 types, found in Appendix~\ref{SportTypes}, so those types, same as "POLITICS" domain, are more specified in 8 types, like SportClub, SportsLeague, SportsTeam, Athlete, Coach, OrganizationMember, SportsManager and SportsEvent. Grouping of types is also shown in appendix~\ref{SportTypes}. This domain is a nice example of that even we retrieve quite a big number of types, we can reduce that number with more specific types which further don't lose the sense of type. For instance "David de Gea" has a type of SoccerPlayer, but after processing will have type of Athlete, which gives sense, because any type of sport player is an athlete.

	At the end we repeat the process for "TRANSPORTATION" domain, where we retrieve in total 86 types. Retrieved types can be found in Appendix~\ref{TransportationTypes}. Those types are after minimized in 14 more specific types like Aircraft, Automobile, Locomotive, MilitaryVehicle, Motorcycle, On-SiteTransportation, Rocket, Ship, SpaceShuttle, SpaceStation, Spacecraft, Train, PublicTransitSystem and Infrastructure. The logic of that who we create more specific ontology types is same as in "POLITICS" or "SPORT" domain.   

	The reason why we group ontology types to more specific ones is that, that when the dataset has a smaller number of types, training a model with Stenford NER is more faster and requires less memory for training. Another reason is faster providing a NER, because is needed to read less types and also the overall results after testing with same data perform better than when ontology types where not grouped.

\section{Data transformation}\label{StanfordNERoutput}
	We define domains as well their types that we will retrieve and process, now we should put everything together and prepare data for Stanford NER application. In Data pre-processing (see Section~\ref{dataPreProcessing}) we explain how we handled the data downloaded from web and we briefly touch how those data will be prepared for training in Stanford NER application. 

	The final thing that is missing is how we will choose which abstracts will be part of our models. Because our goal is to create models with different number of abstracts we need some strict order of retrieved links from DBpedia dataset. The solution that we choose who fits to our requirements is PageRank. PageRank \cite{wiki:PR} is an algorithm used by Google Search to rank websites in their search engine results. So with a prepared and tested SPARQL queries on www.dbpedia.com/sparql and  with help of Apache Jena framework (see Section~\ref{ApacheJena}) we implemented retrieving links, on Java, on DBpedia endpoint\footnote{\url{http://www.dbpedia.com/sparql}}. After retrieving those data, based on their PageRank we search does retrieved link is part on our abstract file. If link is found in nif-abstract dataset it's written to two files, one file is where are written all abstracts from every domain and another file is file for that specific domain. Those files are creating in RDF format, with n-triples, that means that there is subject, in our case that is the link of abstract, then predicate who has isString annotation which tells that next triple contains the abstract text and finally object where abstract text is placed. Next thing that we need to do is to find all annotated words from abstract and their types. The algorithm of finding types is explained in Section~\ref{dataPreProcessing}. What is not mention there is that after finding the types, the abstract is written to file, where on first position is word and on the second position is the type of that word, if there is any, if not the type is O. Final step is to prepare data to be able to train models in Stanford NER with the types that we define in Section~\ref{typesRetrieval}. Because files contains all types that were found on the abstracts we need to clean and group them, as well to create a coarse and fine grained files. The algorithm is very simple, it reads the files who already has all types and if type is part of our retrieved types then either type is leaved as it is, or is grouped to more specific type, for instance if word has type Ambassador, then after filtering that word will have Politician type. The same is for coarse grained annotation, but here proper types after filtering are "POLITICS", "SPORT" or "TRANSPORTATION" type. The whole process is also illustrated at Algorithm~\ref{AlgorithmMain}.
		Interesting fact is that, that when we retrieve links from DBpedia with a specific ontology types types, some links there has types that are not even part of our domain. Here are some interesting links that we catch:
	
	\begin{itemize}
	\item http://dbpedia.org/page/Orbital\_period	
	\item http://dbpedia.org/page/Pregnancy
	\item http://dbpedia.org/page/Melody
	\item http://dbpedia.org/page/ITunes
	\item http://dbpedia.org/page/Tachycardia
	\item http://dbpedia.org/page/Shortwave\_radio
	\item http://dbpedia.org/resource/UTC-05:00
	\end{itemize}
	
	\begin{algorithm}
	Retrieve links from DBpedia NIF Dataset based on their PageRank\;
	\eIf{ Retrieved link is found at nif-abstract dataset}{
		write value from nif-abstract dataset to file
		}{
		go to next retrieved link and repeat steps		
		} 
	Read new file with values from nif-context and get abstract links\;
	Check does that link is consists in nif-text-links dataset\;
	\eIf{link consists in nif-text-links}{
		Get all values (links) from nif-text-links dataset\;
		Search for ontology types in instance-types dataset\;
		\eIf{Link from nif-text-links exists in instance-types}{
			Parse value and return ontology type\;		
		}

		Write abstract text to domain specific file with founded type of the word, as only word and the type at a line\;	
	}{
		Write abstract text to domain specific file with word and O type at a line\;  	
	}
	 
	Read created domain specific files and clean unnecessary types\;
	\eIf{Type equals some of retrieved types}{
		Leave type as it is or group type and write to two domain specific files in coarse and fine grained\; 	
	}{
		Rewrite the type to "O" and write to two domain specific files in coarse and fine grained\;
	}{
		Write to two domain specific files in coarse and fine grained\;	
	}
	\caption{Algorithm for preparing datasets ready for training in Stanford NER\label{AlgorithmMain}}
	\end{algorithm}
	
	   
	
\section{Model generation}\label{modelGeneration}
	With the created files from Section~\ref{StanfordNERoutput} now we can start training models. At Stanford NER CRF FAQ webpage\footnote{\url{https://nlp.stanford.edu/software/crf-faq.html}} is a very nice explanation of that how to train own model with Stanford NER. We follow those steps and used pretty much the same NER properties file with a small correction where we had to add 2 more flags to be able to train big models. Those two flags are saveFeatureIndexToDisk=true, which is used on every properties file and for creating a models in fine grained we use useObservedSequencesOnly=true.Flag saveFeatureIndexToDisk stands for saving the feature name's to disk that aren't actually needed while the core model estimation (optimization) code is run. More interesting is useObservedSequencesOnly flag. It's stands for labeling only adjacent words with label sequences that were seen next to each other in the training data. For some kinds of data this actually gives better accuracy, for other kinds it is worse. After testing on a small model with only 40 abstracts and model with 300 abstracts we find out that for creating a fine grained model with 40 and more abstract this flag gives us better results, while on coarse grained models this flag gives worst results, the exception are models with 500 abstracts where we should use this flag to reduce memory usage. The whole properties file with all used flags can be found in Appendix~\ref{NERpropertiesFile}.  
 
 After creating a properties files, training models is very easy with only one command, where unlike command from Stanford we add Xmx Java option, because standard command use only 4GB of RAM, which for our purposes is not enough for training big models. 
 
 Command for training model ran from the stanford-ner folder:
 \begin{lstlisting}{bash}
 	java -Xmx11g -cp stanford-ner.jar 
 	edu.stanford.nlp.ie.crf.CRFClassifier -prop 
 	locationAndnameOfPropFile.prop
 \end{lstlisting}
	
	\subsection{Training datasets}
	For the aim of our experiments we have trained 57 models. As mentioned earlier for training we have used Stanford NER application explained in Section~\ref{Stanford NER}. We have two types of models, coarse-grained and fine-grained, also those model types are divided in to "POLITICS", "SPORT" or "TRANSPORTATION" specific domains and a global domain who contains all abstracts from every domain. To give an illustration, for dataset with 100 retrieved abstract we will have 4 coarse-grained models (global domain and 3 specific domains), and similarly for a fine-grained models, so in total we have 8 trained models for every dataset. We created 7 different datasets with 10 abstracts, 20 abstracts, 40 abstracts, 100 abstracts, 300 abstracts, 400 abstracts and 500 abstracts. Each of this datasets has 8 trained models and we have one dataset that have also 500 abstracts, but those abstracts are not the same like the previous dataset. This dataset contains abstracts that have lower PageRank value and has only one trained model with abstracts from every domain in fine grained. 

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%NEW CHAPTER%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%CHAPTER 3%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}

There are parameters of the computer used for tests shown in Table \ref{tab:PCparam}.
\begin{table}[H]\centering
	\caption{Testing computer parameters}
	\label{tab:PCparam}
	\begin{tabular}{|l|l|}
	\hline \multicolumn{1}{|c|}{\textbf{Part}} & \multicolumn{1}{|c|}{\textbf{Description}} \\\hline
	CPU & 2.00 GHz Intel(R) Core(TM) i5-4310U \\
	MEM & 16 GB DDR3L\\
	OS & x86\_64 Windows 10 Pro\\
	DISK & 240GB SSD Kingston\\\hline
	\end{tabular}
\end{table}


We have provide various types of experiments. In next sections we will discuss more about every provided experiment. The order of the abstracts is based on PageRank as explained in section~\ref{StanfordNERoutput}.

\section{Goals of the experiments}
	We set a few goals of the experiments. First of all we waned to test does we will get better results if we run the model of all domains in coarse grained, against the model of all domains in fine grained. In this test we run the models also with all domains texts. Then we get those models and we run it with specific domain texts, in both fine and coarse grained. Also we make experiments with specific domain model run with domain specific texts, for example, politics domain model in coarse grained is run with politics domain text also annotated in coarse grained, politics domain model in fine grained is run with politics domain text also annotated in fine grained, and the same for sport and transportation domains.

\section{Evaluation metrics}
	The success of NER systems is exposed to $F_{1}$ score (F-score or F-measure). $F_{1}$ \cite{wiki:F1} score  is a measure of a test's accuracy. It considers both the precision p  and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. 
Written in formula, the $F_{1} =2\cdot \frac{precision \cdot recall}{precision + recall}$.


\section{List of experiments}
	With our trained models we made a few experiments. First one is the model that has 300 abstract on every domain(900 abstract in total). This is our main model and other experiments that we will provide like models that has lower or higher number of abstracts or experiments where model has more abstracts that a test file or vice-verse, all those results will be compared with the results obtained from main experiment. 



%
%Make it like experiment - discussion

%	\begin{figure}[H]\centering
%		\includegraphics[width=\textwidth]{"/500All3DomainsCoarseGrainedRunnedWithAll3DomainsBBCCoarseGrained-tsv".png}
%		\caption{All 3 Domains Coarse BBC Coarse}\label{}
%	\end{figure}
%	
%	\begin{table}[H]\centering
%		\caption{TABLE}
%		\label{}
%		\begin{tabular}{|l|c|r|l|}
%			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
%				POLITICS & 0,9655 & 0,9333 & 0,9492\\
%				SPORT & 1,0000 & 1,0000 & 1,0000\\
%				TRANSPORTATION & 1,0000 & 1,0000 & 1,0000\\\hline
%				Totals & 0,9811 & 0,9630 & 0,9720\\\hline
%		\end{tabular}
%	\end{table}
\subsection{Main experiment}\label{MainExperiment}
	This is our main experiment where other experiments will be compared with this one. This model is trained with top 300 Wikipedia abstracts for every domain. Algorithm for preparing the data for training model explained in section~\ref{StanfordNERoutput}  takes 8622805705290 nanoseconds or 2.40 hours. The model is trained in coarse grained and takes 844.63 seconds in optimization and 873.7 seconds on CRFClassifier training.

\subsubsection{Global domain models}
	First experiment that we do with this model is that we run it with the same text that model is trained in coarse grained. Results are not bad at all, we are above 95\% as shown in Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse}, which is great number for such middle weight model. With such results, someone will say that those are nice results and other experiments will only have a worst results. But let see how model will behaves when we tested with abstracts for every specific domain.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9872 & 0,9462 & 0,9662\\
				SPORT & 0,9846 & 0,9629 & 0,9736\\
				TRANSPORTATION & 0,9940 & 0,9823 & 0,9881\\\hline
				\textbf{Totals} & \textbf{0,9875} & \textbf{0,9625} & \textbf{0,9748}\\\hline
		\end{tabular}
		\caption{Outcomes of base experiment run to be used as reference for subsequential experiments \label{table:All3domainsWithAll3DomiansTop300Coarse}}
	\end{table}	
	
	Table~\ref{table:All3domainsWithPoliticsTop300Coarse} shows the output of model when is tested with abstracts from a "POLITICS" domain. As we said in Section~\ref{StanfordNERoutput} this type of abstract has the biggest word annotation. Result is not even close with the result from previous experiment. Also, trained model annotated words with a "TRANSPORTATION" domain, where the test file don't have any word with that annotation. 
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9839 & 0,4025 & 0,5713\\
				TRANSPORTATION & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9792} & \textbf{0,4025} & \textbf{0,5705}\\\hline
		\end{tabular}
		\caption{Outcomes of base model in coarse grained run with "POLITICS" abstracts\label{table:All3domainsWithPoliticsTop300Coarse}}
	\end{table}	
	
	Table~\ref{table:All3domainsWithSportTop300Coarse} gives us results from abstracts from "SPORT" domain. Here we have the same results like in first experiment, but because trained model annotated some words with a "POLITICS" or "TRANSPORTATION", even those that our test file contains only abstracts from "SPORT" domains and words has only "SPORT" type, the overall result is only a little bit lower that the first experiment.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,0000 & 1,0000 & 0,0000\\
				SPORT & 0,9846 & 0,9628 & 0,9736\\
				TRANSPORTATION & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9819} & \textbf{0,9628} & \textbf{0,9722}\\\hline
		\end{tabular}
		\caption{Outcomes of base model in coarse grained run with "SPORT" abstracts \label{table:All3domainsWithSportTop300Coarse}}
	\end{table}	
	
	Table~\ref{table:All3domainsWithTransportationTop300Coarse} provide outcome with testing with abstracts only from "TRANSPORTATION" domain. As in the previous experiment, the result now is almost the same like in first experiment, but even thought that trained model, as in previous 2 experiments, annotated words with a "SPORT" type, the overall results is better that the experiment where test file contains all abstracts from every domain. 
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 0,9940 & 0,9822 & 0,9880\\\hline
				\textbf{Totals} & \textbf{0,9861} & \textbf{0,9822} & \textbf{0,9841}\\\hline
		\end{tabular}
	\caption{Outcomes of base model in coarse grained run with "TRANSPORTATION" abstracts \label{table:All3domainsWithTransportationTop300Coarse}}
	\end{table}	
	
	In conclusion with this kind of experiments we can say that it is not a good idea to train a model with all chosen domains and then use texts from specific domain to perform NER.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	After we finish the experiments with model that is trained with all abstracts from every domain in coarse grained, we waned to see the impact of model that is trained with same abstracts, but now annotated in fine grained. To train this model we needed 3250.9 seconds from which 3207.45 seconds for optimization. Table~\ref{table:All3domainsWithAll3DomainsTop300Fine} shows the results of provided experiment where we can see that we have a little bit more better total result than experiment in Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse}.


\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Athlete & 1,0000 & 0,9802 & 0,9900\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\ 
				Coach & 1,0000 & 1,0000 & 1,0000\\
				Infrastructure & 1,0000 & 0,9820 & 0,9909\\
				PoliticalParty & 0,9860 & 0,9628 & 0,9743\\
				Politician & 1,0000 & 0,9353 & 0,9665\\
				PublicTransitSystem & 0,9919 & 0,9839 & 0,9879\\
				Ship & 1,0000 & 1,0000 & 1,0000\\
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\ 
				SportsClub & 0,9796 & 0,9683 & 0,9739\\
				SportsEvent & 1,0000 & 0,9242 & 0,9606\\
				SportsLeague & 0,9647 & 0,9805 & 0,9725\\
				SportsManager & 1,0000 & 0,9423 & 0,9703\\
				SportsTeam & 1,0000 & 0,9805 & 0,9902\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9880} & \textbf{0,9712} & \textbf{0,9795}\\\hline
		\end{tabular}
		\caption{Outcomes of base experiment in fine grained run to be used as reference for subsequential experiments \label{table:All3domainsWithAll3DomainsTop300Fine}}
	\end{table}

Then we tested our model with abstracts from "POLITICS" domain. How we can see from Table~\ref{table:All3domainsWithPoliticsTop300Fine} there is some improvements on overall result unlike the experiment in coarse grained, but no satisfying at all. As well table shows that some words again are annotated with types from "SPORT" and "TRANSPORTATION" domain.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,0000 & 0,0000 & 0,0000\\
				PoliticalParty & 0,9860 & 0,9628 & 0,9743\\
				Politician & 1,0000 & 0,1849 & 0,3120\\
				PublicTransitSystem & 0,0000 & 1,0000 & 0,0000\\
				Ship & 0,0000 & 1,0000 & 0,0000\\
				SportsLeague & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9825} & \textbf{0,4072} & \textbf{0,5758}\\\hline
		\end{tabular}
		\caption{Outcomes of base model in fine grained run with "POLITICS" abstracts\label{table:All3domainsWithPoliticsTop300Fine}}
	\end{table}	

    After that we rerun the experiment, but now with abstracts from "SPORT" domain. In Table~\ref{table:All3domainsWithSportTop300Fine} we can see minor growth of the results unlike experiment in Table~\ref{table:All3domainsWithSportTop300Coarse}, but this improvements are so small that are almost unimportant. Also our model annotated some words with types from "POLITICS" and "TRANSPORTATION" domain which the test file don't have those types at all.
    
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 0,9802 & 0,9900\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				Politician & 0,0000 & 1,0000 & 0,0000\\
				SportsClub & 0,9794 & 0,9680 & 0,9737\\
				SportsEvent & 1,0000 & 0,9242 & 0,9606\\
				SportsLeague & 0,9678 & 0,9805 & 0,9741\\
				SportsManager & 1,0000 & 0,9423 & 0,9703\\				
				SportsTeam & 1,0000 & 0,9804 & 0,9901\\
				Train & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9821} & \textbf{0,9716} & \textbf{0,9768}\\\hline
		\end{tabular}
		\caption{Outcomes of base model in fine grained run with "SPORT" abstracts \label{table:All3domainsWithSportTop300Fine}}
	\end{table}
	
    Finally the last experiment with this model are the abstracts from "TRANSPORTATION" domain. Table~\ref{table:All3domainsWithTransportationTop300Fine} shows the output of the provided experiment, where like in previous 2 experiments we can notice a very little improvements on results, from experiment in Table~\ref{table:All3domainsWithTransportationTop300Coarse}, who again can be unimportant. As in previous experiments similarly here model annotated some words with types from other 2 domains, which test file does not even contains.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\				
				Infrastructure & 1,0000 & 0,9820 & 0,9909\\
				Politician & 0,0000 & 1,0000 & 0,0000\\				
				PublicTransitSystem & 0,9918 & 0,9837 & 0,9878\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 0,0000 & 1,0000 & 0,0000\\
				SportsTeam & 0,0000 & 1,0000 & 0,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9862} & \textbf{0,9881} & \textbf{0,9871}\\\hline
		\end{tabular}
		\caption{Outcomes of base model in fine grained run with "TRANSPORTATION" abstracts\label{table:All3domainsWithTransportationTop300Fine}}
	\end{table}	
	
	Provided experiments with the model who is trained with all abstracts from every domain annotated in fine grained, overall provide a very little improvement on results on every experiment. With that observation trained model annotated in fine grained is better to use instead of the model that is annotated in coarse grained. Another benefit of this type of model is that we can see which types are annotated and their results. But, because those improvements are small and is needed almost four times more time to train a fine grained model, maybe the better solution will be a models trained in coarse grained, everything depends on us. Does we want to trained models faster or we want to be more precise. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Evaluation of domain specific models}

	After completing experiments with a global domains in coarse and fine grained, now we will make experiments with models for specific domains.
	
	To train "POLITICS" domain specific model we need 66.7 seconds in total from which 59.53 seconds spend on optimization. In Table~\ref{table:PoliticsdomainsWithPoliticsTop300Coarse} the experiment is provided with model trained only with abstracts from "POLITICS" domain and run with the same texts that model in trained, in coarse grained. The result here is better than experiment in Table~\ref{table:All3domainsWithPoliticsTop300Coarse}, but worse that experiment provided with global domain in Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse}. This can be cause by the fact that model has  biggest number of annotated words.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,8039 & 0,6779 & 0,7355\\\hline
				\textbf{Totals} & \textbf{0,8039} & \textbf{0,6779} & \textbf{0,7355}\\\hline
		\end{tabular}
		\caption{Outcomes of "POLITICS" base model in coarse grained run with "POLITICS" abstracts \label{table:PoliticsdomainsWithPoliticsTop300Coarse}}
	\end{table}
	
	We repeat previous experiment, but now everything in fine grained. Time for training this kind of model i total was 163.5 seconds, from which 155.94 seconds spend on optimization. Table~\ref{table:PoliticsdomainsWithPoliticsTop300Fine} shows that this kind of model provides better result that coarse grained model and the experiment provided in Table~\ref{table:All3domainsWithPoliticsTop300Fine}, but again worst than model trained with all abstracts (see Table~\ref{table:All3domainsWithAll3DomainsTop300Fine}).
	 
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,8240 & 0,6398 & 0,7203\\
				PoliticalParty & 0,8100 & 0,7006 & 0,7513\\
				Politician & 0,8599 & 0,7234 & 0,7858\\\hline
				\textbf{Totals} & \textbf{0,8354} & \textbf{0,6980} & \textbf{0,7606}\\\hline
		\end{tabular}
		\caption{Outcomes of "POLITICS" base model in fine grained run with "POLITICS" abstracts \label{table:PoliticsdomainsWithPoliticsTop300Fine}}
	\end{table}	
	
	In conclusion with provided 2 experiments and from this point of view, for this domain we can say that training a specific model will give better results and will perform faster that global domain tested with text from specific domain. On the other hand the global domain tested with a texts that is trained, how we can see from Table~\ref{table:All3domainsWithAll3DomainsTop300Fine} and Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse} perform even better results that specific trained models.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Next experiment that we do is the same like the previous one, but now the domain is "SPORT". Training time for this model was 93.0 seconds in total, but 82.97 seconds spend on optimization. This model and test file, how in previous one is run with 300 abstracts. Table~\ref{table:SportdomainsWithSportTop300Coarse} shows the outcome of the experiment in coarse grained. From the table we can see that this domain provide a better result that "POLITICS" domain, because here we have less annotated words. But, when compared with base experiment from Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse} and Table~\ref{table:All3domainsWithSportTop300Coarse} those experiments perform better results that this one. 
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,9432 & 0,8839 & 0,9126\\\hline
				\textbf{Totals} & \textbf{0,9432} & \textbf{0,8839} & \textbf{0,9126}\\\hline
		\end{tabular}
		\caption{Outcomes of "SPORT" base model in coarse grained run with "SPORT" abstracts \label{table:SportdomainsWithSportTop300Coarse}}
	\end{table}

Also we train a model in fine grained, with total time of 554.9 seconds, with 543.55 seconds spend on optimization and provide an experiment. Table~\ref{table:SportdomainsWithSportTop300Fine} show that the result is little bit more better that result with model in coarse grained, but still this result is lower that the results for Table~\ref{table:All3domainsWithAll3DomainsTop300Fine} and Table~\ref{table:All3domainsWithSportTop300Fine}.
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 0,9713 & 0,8366 & 0,8989\\
				Coach & 1,0000 & 0,7500 & 0,8571\\
				SportsClub & 0,9453 & 0,9041 & 0,9242\\
				SportsEvent & 1,0000 & 0,7879 & 0,8814\\
				SportsLeague & 0,9418 & 0,8958 & 0,9182\\
				SportsManager & 1,0000 & 0,9615 & 0,9804\\				
				SportsTeam & 0,9845 & 0,8301 & 0,9007\\\hline
				\textbf{Totals} & \textbf{0,9592} & \textbf{0,8750} & \textbf{0,9152}\\\hline
		\end{tabular}
		\caption{Outcomes of "SPORT" base model in fine grained run with "SPORT" abstracts \label{table:SportdomainsWithSportTop300Fine}}
	\end{table}
	
	After provided 2 experiments with trained models for specific domain, the results shows that training a global model will perform better result than training a domain specific model.	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Final experiment that we do with this size of abstracts (300 abstracts) is with "TRANSPORTATION" domain. We needed 58.9 seconds to train the model, from which 50.35 seconds on optimization.
  Table~\ref{table:TransportationDomainsWithTransportationTop300Coarse} show the experiment outcome in coarse grained, where we can see that this result is lower than results from experiments provided in Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse} and Table~\ref{table:All3domainsWithTransportationTop300Coarse}.  
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 0,9583 & 0,9109 & 0,9340\\\hline
				\textbf{Totals} & \textbf{0,9583} & \textbf{0,9109} & \textbf{0,9340}\\\hline
		\end{tabular}
		\caption{Outcomes of "TRANSPORTATION" base model in coarse grained run with "TRANSPORTATION" abstracts \label{table:TransportationDomainsWithTransportationTop300Coarse}}
	\end{table}
	
	Finally we make an experiment in fine grained. Total training time was 702.6 seconds, from which 686.50 seconds spend on optimization. In Table~\ref{table:TransportationDomainsWithTransportationTop300Fine} we can see the results of provided experiment, where those results are even worse that the experiment with coarse grained model, which in previous two domain, "SPORT" and "POLITICS" was not that case. Also those results are worse than the experiments with a global domain in Table~\ref{table:All3domainsWithAll3DomainsTop300Fine} and Table~\ref{table:All3domainsWithTransportationTop300Fine}. 
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,9659 & 0,8333 & 0,8947\\
				Automobile & 1,0000 & 0,8000 & 0,8889\\				
				Infrastructure & 0,9550 & 0,9550 & 0,9550\\
				PublicTransitSystem & 0,9662 & 0,9309 & 0,9482\\
				Ship & 1,0000 & 0,6429 & 0,7826\\				
				SpaceShuttle & 1,0000 & 0,8333 & 0,9091\\
				SpaceStation & 0,0000 & 1,0000 & 0,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9660} & \textbf{0,9010} & \textbf{0,9324}\\\hline
		\end{tabular}
		\caption{Outcomes of "TRANSPORTATION" base model in fine grained run with "TRANSPORTATION" abstracts \label{table:TransportationDomainsWithTransportationTop300Fine}}
	\end{table}	

	In conclusion with the provided experiments in this section, we can say that training a global model and providing a NER is a better, but a little bit slowest solution than training a domain specific model, except the "POLITICS" domain, where the results was better in domain specific model unlike the experiment with a global domain and test file with "POLITICS" abstracts, but worse than experiment with a global domain tested with abstracts from all 3 domains. Then we waned to see the impact of fine grained trained models, where in most of the cases this kind of models provide a better results than models trained in coarse grained, except the experiment in "TRANSPORTATION" specific domain where the coarse grained model was better that fine grained model.
	
	After we finish with the main experiment, we were interested about the impact of the size of abstracts that will be used for training models. Next two subsections show the behavior of trained models.     
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiments that has less than 300 abstracts in model}\label{}
	In this subsection we want to see the behavior of models that are trained with less than 300 abstracts. First experiment is trained with 10 abstracts, then we have experiments with 20 abstracts, next experiment with 40 abstracts, and finally experiment with 100 abstracts. The order of abstracts, how we said earlier, is based on PageRank.
	
	\textbf{Model trained with 10 abstracts to every domain.} To retrieve links from DBpedia with SPARQL and prepare data to be able to train models with 10 abstract, our algorithm explain in Section~\ref{StanfordNERoutput} takes in total 10.81 minutes, which comparing with main experiment, where we need 2.40 hours, is way more faster to prepare data. Of coarse this indicates that training models will also be faster than in main experiment.
	
	\textbf{Coarse grained model.} How in the main experiment, also here we start with model trained in coarse grained. To train this kind of model we need 19.6 seconds, from which 17.44 seconds spend on optimization. From Table~\ref{table:GlobalDomainWithAllAbstractsTop10Coarse} we see that trained model perform the best results without any loosing of words in "SPORT" and "TRANSPORTATION" domains, but worst result in "POLITICS" domain. The result of "POLITICS" domain is even worst than the result from main experiment provided in Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse}. Because of this, there is a little bit lower overall result than in the main experiment. This can indicates that training models with lowest number of abstracts, for this kind of domains, is not worth. But let's see how model will behaves when is tested with abstracts from a specific domains.
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9655 & 0,9333 & 0,9492\\
				SPORT & 1,0000 & 1,0000 & 1,0000\\
				TRANSPORTATION & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9811} & \textbf{0,9630} & \textbf{0,9720}\\\hline
		\end{tabular}
	\caption{Outcomes of global model in coarse grained run with 10 abstracts from every domain \label{table:GlobalDomainWithAllAbstractsTop10Coarse}}
	\end{table}

	Table~\ref{table:GlobalDomainWithPoliticsTop10Coarse} show the output of experiment where we have global model that is tested with 10 abstracts from "POLITICS" domain. Here model do not annotated any word from other domains unlike in the main experiment in Table~\ref{table:All3domainsWithPoliticsTop300Coarse}, but even this and the fact that here are much less abstracts does not help to provide a better results.
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9655 & 0,3636 & 0,5283\\\hline
				%SPORT & 1,0000 & 1,0000 & 1,0000\\\hline
				%TRANSPORTATION & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9655} & \textbf{0,3636} & \textbf{0,5283}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in coarse grained run with 10 abstracts from "POLITICS" domain \label{table:GlobalDomainWithPoliticsTop10Coarse}}
	\end{table}

	Then we test our global model with abstracts from "SPORT" domain. Table~\ref{table:GlobalDomainWithSportTop10Coarse} show the outcome of the experiment. We can see that model perform perfect result, how in experiment in Table~\ref{table:GlobalDomainWithAllAbstractsTop10Coarse} without any misleading annotations, which we cannot say for the main experiment where model annotated words from "POLITICS" and "TRANSPORTATION" domains. 
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in coarse grained run with 10 abstracts from "SPORT" domain \label{table:GlobalDomainWithSportTop10Coarse}}	
	\end{table}	

	Finally we tested the model with abstract from "TRANSPORTATION" domain. From Table~\ref{table:GlobalDomainWithTransportationTop10Coarse} we can see that model as well as in previous experiment perform maximum result without misleading annotations, unlike the main experiment where how we can see from Table~\ref{table:All3domainsWithTransportationTop300Coarse} model annotate words with "SPORT" domain and has a lowest result than this one.
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in coarse grained run with 10 abstracts from "TRANSPORTATION" domain \label{table:GlobalDomainWithTransportationTop10Coarse}}	
	\end{table}

	In conclusion with the results from provided experiments we see that there is a huge impact of the number of abstracts for training a global model in coarse grained. We see that for "SPORT" and "TRANSPORTATION" domain our model provide maximum results, which is what we want to reach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	

	\textbf{Fine grained model.} After we finish the experiments with global models in coarse grained, we waned to see the impact of fine grained model. Does also here this kind of model will perform better results as was the case in main experiment, where global fine grained model perform a slide better results.
	
	Training a fine grained model takes in total 124.7 seconds, from which 120,73 seconds spent in optimization. From Table~\ref{table:GlobalDomainWithAllAbstractsTop10Fine} we can see that now fine grained model provide exactly the same overall result as well as coarse grained model. Also from table we can see in which ontology type our model fails to perform maximum result. So, because of PoliticalParty type where we have a lowest result, the total result is not at the maximum level, even thought other ontology types has maximum annotation.
	  	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				PoliticalParty & 0,9600 & 0,9231 & 0,9412\\
				Politician & 1,0000 & 1,0000 & 1,0000\\
				PublicTransitSystem & 1,0000 & 1,0000 & 1,0000\\
				Ship & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9811} & \textbf{0,9630} & \textbf{0,9720}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 10 abstracts from every domain \label{table:GlobalDomainWithAllAbstractsTop10Fine}}	
	\end{table}

	Then how in previous experiments, we take the global train model and test it with abstracts from every specific domain separately. 
	The first domain abstracts was from "POLITICS" domain, where from Table~\ref{table:GlobalDomainWithPoliticsTop10Fine} we can see that model perform same result as well as in coarse grained model experiment in Table~\ref{table:GlobalDomainWithPoliticsTop10Coarse}. Also even our model and test files has words with Election ontology type, the model do not recognize any of them. With that misleading we have lower results, if that doesn't happens the model will perform pretty mach good recognition.
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,0000 & 0,0000 & 0,0000\\
				PoliticalParty & 0,9600 & 0,9231 & 0,9412\\
				Politician & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9655} & \textbf{0,3636} & \textbf{0,5283}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 10 abstracts from "POLITICS" domain \label{table:GlobalDomainWithPoliticsTop10Fine}}		
	\end{table}	

	Then we test it our model with abstracts from "SPORT" domain. Table~\ref{table:GlobalDomainWithSportTop10Fine} shows that our model recognize all annotated words from test file without any misleading and perform maximum F1 score. In comparing with the main experiment in Table~\ref{table:All3domainsWithSportTop300Fine} where we have some loosing, here that is not the case and it is what we want to reach.
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 10 abstracts from "SPORT" domain \label{table:GlobalDomainWithSportTop10Fine}}		
	\end{table}
	
	Final experiment that we do with global train model was with "TRANSPORTATION" domain abstracts. In Table~\ref{table:GlobalDomainWithTransportationTop10Fine} we can see that model, same as in previous experiment with "SPORT" abstracts, perform maximum F1 score result, which in comparing with the main experiment from Table~\ref{table:All3domainsWithTransportationTop300Fine} here we have improvements on result.
	 
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				PublicTransitSystem & 1,0000 & 1,0000 & 1,0000\\
				Ship & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 10 abstracts from "TRANSPORTATION" domain \label{table:GlobalDomainWithTransportationTop10Fine}}		
	\end{table}
	
	In conclusion from the provided experiments where we had 10 abstracts on every domain, in comparing with the main experiments, we can say that there is an impact on performing a NER with a smallest number of abstracts for training a testing models. Here when we use coarse of fine grained global model and test it with texts from specific domain, except the "POLITICS" domain abstracts, on other two domain, model perform NER without any misleading, which is what we waned to reach. Also training such small models takes way more less time, than training a big models. 
 
 \subsubsection{Evaluation of domain specific models}
	In next 6 experiments trained models has 10 domain specific abstracts per model and also test files have the same specification.
	
	\textbf{"POLITICS" specific domain.} First domain that we provide an experiment was "POLITICS" specific domain. To train this model we need 3.8 seconds, from which 2.56 seconds spent in optimization. Table~\ref{table:PoliticsDomainWithPoliticsTop10Coarse} show the outcome of the experiment, where the result here is way better, than with comparing with main experiment in Table~\ref{table:PoliticsdomainsWithPoliticsTop300Coarse} and the experiment with global train model tested with "POLITICS" domain specific text in Table~\ref{table:GlobalDomainWithPoliticsTop10Coarse}.  

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9737 & 0,9610 & 0,9673\\\hline
				\textbf{Totals} & \textbf{0,9737} & \textbf{0,9610} & \textbf{0,9673}\\\hline
		\end{tabular}
		\caption{Outcome of "POLITICS" domain specific model in coarse grained run with 10 abstracts from the same domain \label{table:PoliticsDomainWithPoliticsTop10Coarse}}
	\end{table}	
	
	Because we want to know the impact when model is trained in fine grained, we make an experiment with fine grained model. For training this model we need 8.3 seconds, from which 6.99 seconds spent in optimization. From Table~\ref{table:PoliticsDomainWithPoliticsTop10Fine} we can see that this kind of model provide a higher result than coarse grained model from previous experiment. Also this result is better than result from main experiment in Table~\ref{table:PoliticsdomainsWithPoliticsTop300Fine} and the result from the experiment where we tested the global trained model with domain specific text in Table~\ref{table:GlobalDomainWithPoliticsTop10Fine}.
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 1,0000 & 0,9333 & 0,9655\\
				PoliticalParty & 0,9600 & 0,9231 & 0,9412\\
				Politician & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9867} & \textbf{0,9610} & \textbf{0,9737}\\\hline
		\end{tabular}
		\caption{Outcome of "POLITICS" domain specific model in fine grained run with 10 abstracts from the same domain \label{table:PoliticsDomainWithPoliticsTop10Fine}}		
	\end{table}	

	From the "POLITICS" domain specific experiments we can say that for this kind of domain with a lower number of abstracts for training a model the application provide NER with better results unlike the same experiments from the main experiment, where we have a worst results than here.
		
	
	\textbf{"SPORT" specific model.} Training for a "SPORT" domain coarse grained model with 10 abstracts we need 5.0 seconds, from which 3.50 seconds spend on optimization. From Table~\ref{table:SportDomainWithSportTop10Coarse} is clear that this model, same as experiment in Table~\ref{table:GlobalDomainWithSportTop10Coarse}, provide excellent result unlike the main experiment in Table~\ref{table:SportdomainsWithSportTop300Coarse} where we have some loosing in recognition. 
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcome of "SPORT" domain specific model in coarse grained run with 10 abstracts from the same domain \label{table:SportDomainWithSportTop10Coarse}}
	\end{table}	

	Same as in the previous experiment, also here we have a fine grained model. Time needed for training this model was 26.4 seconds, from which 24.64 seconds spent in optimization. From Table~\ref{table:SportDomainWithSportTop10Fine} we see that the result is exactly the same like in coarse grained model (see Table~\ref{table:SportDomainWithSportTop10Coarse}) and the experiment from Table~\ref{table:GlobalDomainWithSportTop10Fine} where model is trained with abstracts from every domain and test file contains only abstracts from "SPORT" domain. Those results from Table~\ref{table:SportDomainWithSportTop10Fine} are of coarse better that the results from the main experiment in Table~\ref{table:SportdomainsWithSportTop300Fine}, because here we don't have any false recognition.
	 
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcome of "SPORT" domain specific model in fine grained run with 10 abstracts from the same domain \label{table:SportDomainWithSportTop10Fine}}
	\end{table}	

	From the experiments provided in Table~\ref{table:SportDomainWithSportTop10Coarse} and Table~\ref{table:SportDomainWithSportTop10Fine} as well as in previous experiment the number of abstracts needed for training a model plays significant role also in "SPORT" domain. Here as well there is no difference if model is trained in coarse or fine grain, because we have the same result, but here plays role the time needed for training those models.
	
	\textbf{"TRANSPORTATION" specific model.} Finally we have "TRANSPORTATION" domain. For training a coarse grain model we needed 4.3 seconds, from which 3.10 seconds spent in optimization. From Table~\ref{table:TransportationDomainWithTransportationTop10Coarse} we can see that as well as in "SPORT" specific model NER is provided without any wrong recognition, which was also the case in experiment from Table~\ref{table:GlobalDomainWithTransportationTop10Coarse}. This means that this model also turned out to be better than the main experiment who has 300 abstracts (see Table~\ref{table:TransportationDomainsWithTransportationTop300Coarse}.
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcome of "TRANSPORTATION" domain specific model in coarse grained run with 10 abstracts from the same domain \label{table:TransportationDomainWithTransportationTop10Coarse}}
	\end{table}

	With total time of 14.3 seconds, from which 12.99 seconds spent on optimization we trained a fine grained model. Table~\ref{table:TransportationDomainWithTransportationTop10Fine} shows that this model 100\% precise same as the coarse grain model (see Table~\ref{table:TransportationDomainWithTransportationTop10Fine}) and the experiment where model is trained with abstracts from every domain and test file contains only abstract from "TRANSPORTATION" domain (see Table~\ref{table:GlobalDomainWithTransportationTop10Fine}). Of coarse this experiment provide a better result that the main experiment in Table~\ref{table:TransportationDomainsWithTransportationTop300Fine}.   
   
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				PublicTransitSystem & 1,0000 & 1,0000 & 1,0000\\
				Ship & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcome of "TRANSPORTATION" domain specific model in fine grained run with 10 abstracts from the same domain \label{table:TransportationDomainWithTransportationTop10Fine}}
	\end{table}

	"TRANSPORTATION" domain model as well as previous two domain models provide a better NER than the main experiment, but of coarse here we have much less annotated words in model and if we test this model with some other data, the results will be worst than in the main experiment where we have much more data.
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Evaluation of global domain}
\textbf{Datasets with 20 abstracts for every domain.} Because we waned to know the impact of train data, we decided to increase retrieved abstract from DBpedia to 20 abstracts per domain. Time need to retrieved those abstracts and prepare datasets for training in Stanford NER was 21.30 minutes.

\textbf{Description of the experiment:} In Table~\ref{table:GlobalDomainWithAllAbstractsTop20Coarse} we provide an experiment where the model was trained with abstracts from every domain, in total 60 abstracts, annotated in coarse grain. We need 36.6 seconds to train the model, from which 32.52 seconds spent in optimization. The model was tested with the same dataset that was trained.    

\textbf{Results of the experiment:} Table~\ref{table:GlobalDomainWithAllAbstractsTop20Coarse} show the output of the experiment, where the overall precision is on maximum level, the recall on "POLITICS" entities is a little bit lower, which results with overall lower recall and not bad at all F1 overall score. For the "SPORT" and "TRANSPORTATION" entities we have a maximum recognition. Referring to the main experiment from Table~\ref{table:All3domainsWithAll3DomiansTop300Coarse} is clearly that results here are better than in main experiment.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 1,0000 & 0,9615 & 0,9804\\
				SPORT & 1,0000 & 1,0000 & 1,0000\\
				TRANSPORTATION & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9780} & \textbf{0,9889}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in coarse grained run with 20 abstracts from every domain \label{table:GlobalDomainWithAllAbstractsTop20Coarse}}
	\end{table}

\textbf{Description of the experiment:} For purposes of the experiment in Table~\ref{table:GlobalDomainWithPoliticsTop20Coarse} we have use the same global trained model from the previous experiment, but now the test file contains only 20 abstracts from "POLITICS" domain. 

\textbf{Results of the experiment:} From Table~\ref{table:GlobalDomainWithPoliticsTop20Coarse} we see that the result is way more worst than the previous experiment, but thanks to maximum precision and not recognizing any entity from other domains is slightly better than main experiment from Table~\ref{table:All3domainsWithPoliticsTop300Coarse} where model recognize entity from "TRANSPORTATION" domain.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 1,0000 & 0,3906 & 0,5618\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,3906} & \textbf{0,5618}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in coarse grained run with 20 abstracts from "POLITICS" domain \label{table:GlobalDomainWithPoliticsTop20Coarse}}
	\end{table}

\textbf{Description of the experiment:} This experiment is almost identical like previous one, with only difference is test file, where now we tested with abstracts from "SPORT" domain.

\textbf{Results of the experiment:} From Table~\ref{table:GlobalDomainWithAllAbstractsTop20Coarse} we see that model provide maximum recognition without any wrong entity recognition of other domains. But this is not the case in the main experiment from Table~\ref{table:All3domainsWithSportTop300Coarse} where also recognize entities from other two domains, even the file contains only abstracts from "SPORT" domain. 

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in coarse grained run with 20 abstracts from "SPORT" domain \label{table:GlobalDomainWithSportTop20Coarse}}
	\end{table}	

\textbf{Description of the experiment:} The final experiment is also the same like previous two, where now test file contains only abstracts from "TRANSPORTATION" domain.

\textbf{Results of the experiment:} Table~\ref{table:GlobalDomainWithTransportationTop20Coarse} shows that for "TRANSPORTATION" entities we have maximum recognition, but model also make a wrong entity recognition from "SPORT" domain. This makes overall result not to be on his maximum and also in comparing with the main experiment from Table~\ref{table:All3domainsWithTransportationTop300Coarse} where the model also recognize entity from "SPORT" domain, here the overall result is worst. 

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9231} & \textbf{1,0000} & \textbf{0,9600}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in coarse grained run with 20 abstracts from "TRANSPORTATION" domain \label{table:GlobalDomainWithTransportationTop20Coarse}}
	\end{table}	

\textbf{Description of the experiment:} Experiment in Table~\ref{table:GlobalDomainWithAllAbstractsTop20Fine} is provided with same data like the experiment in Table~\ref{table:GlobalDomainWithAllAbstractsTop20Coarse}, but now the model and test data are annotated in fine grained. We needed in total 239.1 seconds to train model, from which 233.18 seconds spent in optimization.

\textbf{Results of the experiment:} How we can see from Table~\ref{table:GlobalDomainWithAllAbstractsTop20Fine} our model provide maximum precision, but because there are 2 ontology types from "TRANSPORTATION" domain, where out model provide a half on the maximum in the recall we have a lower result at the end. Also in comparing with the main experiment from Table~\ref{table:All3domainsWithAll3DomainsTop300Fine} we have a slightly lower results here. As well those results are lower than the experiment in coarse grain (see Table~\ref{table:GlobalDomainWithAllAbstractsTop20Coarse}).

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,5000 & 0,6667\\
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				Infrastructure & 1,0000 & 0,5000 & 0,6667\\
				PoliticalParty & 1,0000 & 0,9512 & 0,9750\\
				Politician & 1,0000 & 1,0000 & 1,0000\\
				Ship & 1,0000 & 1,0000 & 1,0000\\
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\ 
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9560} & \textbf{0,9775}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 20 abstracts from every domain \label{table:GlobalDomainWithAllAbstractsTop20Fine}}
	\end{table}

\textbf{Description of the experiment:} In this experiment we use the same trained model from previous experiment, but now the test file contains only abstracts from "POLITICS" domain, who is annotated in fine grain.

\textbf{Results of the experiment:} Table~\ref{table:GlobalDomainWithPoliticsTop20Fine} show the output of the experiment, where we can see that even we have Election type on model and test file, the model do not find any entity with that type. Also for the Politician type we have a very low recall, which reflects that there is a very low overall result. In comparing with the experiment in coarse grain (see Table~\ref{table:GlobalDomainWithPoliticsTop20Coarse} we have exactly the same overall result, but when we compare with the main experiment from Table~\ref{table:All3domainsWithPoliticsTop300Fine} even in that experiment model also annotate some words from other two domains, the overall result is better than the result here.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,0000 & 0,0000 & 0,0000\\
				PoliticalParty & 1,0000 & 0,9512 & 0,9750\\
				Politician & 1,0000 & 0,2000 & 0,3333\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,3906} & \textbf{0,5618}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 20 abstracts from "POLITICS" domain \label{table:GlobalDomainWithPoliticsTop20Fine}}
	\end{table}	

\textbf{Description of the experiment:} How in the previous experiment also here we have the same model but now tested with abstracts from "SPORT" domain annotated in fine grain.

\textbf{Results of the experiment:} In Table~\ref{table:GlobalDomainWithSportTop20Fine} we have the output of the provided experiment. How we can see the results are excellent, there is no any wrong recognition or some lower values on precision and recall. Which in comparing with experiment in coarse grain (see Table~\ref{table:GlobalDomainWithSportTop20Coarse}) we have the same overall result, but we cannot say that about the results from the main experiment provided in Table~\ref{table:All3domainsWithSportTop300Fine} where we have wrong recognition of entities from other 2 domains and only one type has maximum precision and recall.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 20 abstracts from "SPORT" domain \label{table:GlobalDomainWithSportTop20Fine}}
	\end{table}	

\textbf{Description of the experiment:} The last experiment with the model used in previous 3 experiment is now the dataset test file with abstracts from "TRANSPORTATION" domain also annotated in fine grain.

\textbf{Results of the experiment:} Table~\ref{table:GlobalDomainWithTransportationTop20Fine} shows that our train model provide the same results how in experiment in Table~\ref{table:GlobalDomainWithAllAbstractsTop20Fine} for the "TRANSPORTATION" ontology types. Also we have exactly the same overall result with the experiment in coarse grain (see Table~\ref{table:GlobalDomainWithTransportationTop20Coarse}, but when we compare results with the main experiment from Table~\ref{table:All3domainsWithTransportationTop300Fine} we have a way more better results than here, even thought that there model recognize some entities from other two domains.

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,5000 & 0,6667\\
				Infrastructure & 1,0000 & 0,5000 & 0,6667\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9091} & \textbf{0,8333} & \textbf{0,8696}\\\hline
		\end{tabular}
		\caption{Outcomes of global model in fine grained run with 20 abstracts from "TRANSPORTATION" domain \label{table:GlobalDomainWithTransportationTop20Fine}}		
	\end{table}

	%After we finish the experiments with models that are trained with texts from every domain, we do tests with models that are trained with texts from a particular domain (politics, sport and transportation).
	%Figure~\ref{img:PoliticsWithPoliticsCoarseTop20} shows the output of politics coarse grain specified domain, where we can see that we have a way more better results than in experiment with a model trained with texts from all 3 domains (see Figure~\ref{img:All3DomainWithAll3DomainCoarseTop20} and Figure~\ref{img:All3DomainWithPoliticstCoarseTop20}). The politics specific domain finds 125 true positive words unlike the experiments with all 3 domains that gives us only 50 true positive annotated words. However we repeated this experiment, but now with model trained in fine grained. Figure~\ref{img:PoliticsWithPoliticsFineTop20} shows the result of experiment, where we can see that there is no false positive annotated word like in previous experiment (see Figure~\ref{img:PoliticsWithPoliticsCoarseTop20}), which result with a better F1 value in total.  
\subsubsection{Evaluation of domain specific models}
\textbf{Description of the experiment:}

\textbf{Results of the experiment:}
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9921 & 0,9766 & 0,9843\\\hline
				\textbf{Totals} & \textbf{0,9655} & \textbf{0,3636} & \textbf{0,5283}\\\hline
		\end{tabular}
		\caption{Outcome of "POLITICS" domain specific model in coarse grained run with 20 abstracts from the same domain \label{table:PoliticsDomainWithPoliticsTop20Coarse}}		
	\end{table}	
	
\textbf{Description of the experiment:}

\textbf{Results of the experiment:}
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 1,0000 & 0,9688 & 0,9841\\
				PoliticalParty & 1,0000 & 0,9512 & 0,9750\\
				Politician & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9766} & \textbf{0,9881}\\\hline
		\end{tabular}
		\caption{Outcome of "POLITICS" domain specific model in fine grained run with 20 abstracts from the same domain \label{table:PoliticsDomainWithPoliticsTop20Fine}}	
	\end{table}
	
	%Sport specific domain do not disappointed us. Figure~\ref{img:SportWithSportCoarseTop20} show the output of experiment in coarse grained and Figure~\ref{img:SportWithSportFineTop20} show the output of experiment in fine grained. Both experiment same as experiments with all 3 domains together (see Figure~\ref{img:All3DomainWithAll3DomainCoarseTop20}, Figure~\ref{img:All3DomainWithSportCoarseTop20}, Figure~\ref{img:All3DomainWithAll3DomainFineTop20}, Figure~\ref{img:All3DomainWithSportFineTop20}) give us perfect results. But if you want to be more precise, we recommend to use the specific model, because is fastest of reading words.  
\textbf{Description of the experiment:}

\textbf{Results of the experiment:}
	
	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcome of "SPORT" domain specific model in coarse grained run with 20 abstracts from the same domain \label{table:SportDomainWithSportTop20Coarse}}			
	\end{table}	
	
\textbf{Description of the experiment:}

\textbf{Results of the experiment:}	

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
		\caption{Outcome of "SPORT" domain specific model in fine grained run with 20 abstracts from the same domain \label{table:SportDomainWithSportTop20Fine}}			
	\end{table}	
	
	%At the end we make experiments with a transportation specific domain. In Figure~\ref{img:TransportationWithTransportationCoarseTop20} we can see that our model gives us a worst results that experiment in all 3 domains. For instance experiment in Figure~\ref{img:All3DomainWithAll3DomainCoarseTop20} gives us a perfect annotation with 1.0000 value at F1, also experiment in Figure~\ref{img:All3DomainWithTransportationCoarseTop10} gives the same result like previous experiment, but here because of false positive annotated word in SPORT domain in total we have a lower value in F1. In overall experiments in all 3 domains gives a better result that the experiment in specific domain.
	%Of coarse we make an experiment with model that is trained with fine grained text. How Figure~\ref{img:TransportationWithTransportationFineTop20} shows that this model even gives worst result that the experiment with coarse grained model, and also worst results than the experiment in Figure~\ref{img:All3DomainWithTransportationCoarseTop20}.		
	
\textbf{Description of the experiment:}

\textbf{Results of the experiment:}

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 1,0000 & 0,8333 & 0,9091\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,8333} & \textbf{0,9091}\\\hline
		\end{tabular}
		\caption{Outcome of "TRANSPORTATION" domain specific model in coarse grained run with 20 abstracts from the same domain \label{table:TransportationDomainWithTransportationTop20Coarse}}			
	\end{table}	
	
\textbf{Description of the experiment:}

\textbf{Results of the experiment:}	

	\begin{table}[H]\centering
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,5000 & 0,6667\\
				Infrastructure & 1,0000 & 0,5000 & 0,6667\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,7500} & \textbf{0,8571}\\\hline
		\end{tabular}
		\caption{Outcome of "TRANSPORTATION" domain specific model in fine grained run with 20 abstracts from the same domain \label{table:TransportationDomainWithTransportationTop20Fine}}			
	\end{table}
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Top 40 Links}
	
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9890 & 0,9375 & 0,9626\\
				SPORT & 1,0000 & 1,0000 & 1,0000\\
				TRANSPORTATION & 1,0000 & 0,9846 & 0,9922\\\hline
				\textbf{Totals} & \textbf{0,9960} & \textbf{0,9724} & \textbf{0,9841}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9890 & 0,3529 & 0,5202\\\hline
				\textbf{Totals} & \textbf{0,9890} & \textbf{0,3529} & \textbf{0,5202}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 1,0000 & 0,9846 & 0,9922\\\hline
				\textbf{Totals} & \textbf{0,9697} & \textbf{0,9846} & \textbf{0,9771}\\\hline
		\end{tabular}
	\end{table}	
		
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				Infrastructure & 1,0000 & 1,0000 & 1,0000\\
				PoliticalParty & 0,9863 & 0,9730 & 0,9796\\
				Politician & 1,0000 & 0,8182 & 0,9000\\
				PublicTransitSystem & 1,0000 & 1,0000 & 1,0000\\
				Ship & 1,0000 & 1,0000 & 1,0000\\
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\ 
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9960} & \textbf{0,9764} & \textbf{0,9861}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,0000 & 0,0000 & 0,0000\\
				PoliticalParty & 0,9863 & 0,9730 & 0,9796\\
				Politician & 1,0000 & 0,1565 & 0,2707\\\hline
				\textbf{Totals} & \textbf{0,9890} & \textbf{0,3529} & \textbf{0,5202}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 1,0000 & 1,0000\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Infrastructure & 1,0000 & 1,0000 & 1,0000\\
				PublicTransitSystem & 1,0000 & 1,0000 & 1,0000\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9701} & \textbf{1,0000} & \textbf{0,9848}\\\hline
		\end{tabular}
	\end{table}	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9921 & 0,9804 & 0,9862\\\hline
				\textbf{Totals} & \textbf{0,9921} & \textbf{0,9804} & \textbf{0,9862}\\\hline
		\end{tabular}
	\end{table}
		
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 1,0000 & 0,9848 & 0,9924\\
				PoliticalParty & 0,9863 & 0,9730 & 0,9796\\
				Politician & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9960} & \textbf{0,9882} & \textbf{0,9921}\\\hline
		\end{tabular}
	\end{table}
			
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{1,0000} & \textbf{1,0000}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 1,0000 & 1,0000\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 0,9474 & 0,9730\\
				SportsEvent & 1,0000 & 1,0000 & 1,0000\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9890} & \textbf{0,9945}\\\hline
		\end{tabular}
	\end{table}	

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 1,0000 & 0,9846 & 0,9922\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9846} & \textbf{0,9922}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Infrastructure & 1,0000 & 1,0000 & 1,0000\\
				PublicTransitSystem & 1,0000 & 0,9630 & 0,9811\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9846} & \textbf{0,9922}\\\hline
		\end{tabular}
	\end{table}	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Top 100 Links}


	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9920 & 0,9612 & 0,9764\\
				SPORT & 0,9963 & 0,9926 & 0,9944\\
				TRANSPORTATION & 1,0000 & 0,9735 & 0,9865\\\hline
				\textbf{Totals} & \textbf{0,9952} & \textbf{0,9766} & \textbf{0,9856}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9920 & 0,3615 & 0,5299\\\hline
				\textbf{Totals} & \textbf{0,9920} & \textbf{0,3615} & \textbf{0,5299}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,9962 & 0,9888 & 0,9925\\\hline
				\textbf{Totals} & \textbf{0,9962} & \textbf{0,9888} & \textbf{0,9925}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 1,0000 & 0,9735 & 0,9865\\\hline
				\textbf{Totals} & \textbf{0,9821} & \textbf{0,9735} & \textbf{0,9778}\\\hline
		\end{tabular}
	\end{table}	
		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,6957 & 0,8205\\
				Athlete & 1,0000 & 0,4167 & 0,5882\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\ 
				Coach & 1,0000 & 0,6667 & 0,8000\\
				Infrastructure & 1,0000 & 1,0000 & 1,0000\\
				PoliticalParty & 0,8774 & 0,6700 & 0,7598\\
				Politician & 1,0000 & 0,7455 & 0,8542\\
				PublicTransitSystem & 0,9744 & 0,7308 & 0,8352\\
				Ship & 1,0000 & 0,6000 & 0,7500\\
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\ 
				SportsClub & 0,9512 & 0,9398 & 0,9455\\
				SportsEvent & 0,9737 & 0,8605 & 0,9136\\
				SportsLeague & 0,9500 & 0,8636 & 0,9048\\
				SportsManager & 1,0000 & 1,0000 & 1,0000\\
				SportsTeam & 1,0000 & 0,6364 & 0,7778\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9452} & \textbf{0,7535} & \textbf{0,8385}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,0000 & 0,0000 & 0,0000\\
				PoliticalParty & 0,8774 & 0,6700 & 0,7598\\
				Politician & 1,0000 & 0,1285 & 0,2278\\
				SportsEvent & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,8985} & \textbf{0,2580} & \textbf{0,4009}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 0,4167 & 0,5882\\
				Coach & 1,0000 & 0,6667 & 0,8000\\
				SportsClub & 0,9506 & 0,9390 & 0,9448\\
				SportsEvent & 1,0000 & 0,8605 & 0,9250\\
				SportsLeague & 0,9500 & 0,8636 & 0,9048\\
				SportsManager & 1,0000 & 1,0000 & 1,0000\\				
				SportsTeam & 1,0000 & 0,6250 & 0,7692\\\hline
				\textbf{Totals} & \textbf{0,9683} & \textbf{0,7985} & \textbf{0,8753}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,6957 & 0,8205\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\				
				Infrastructure & 1,0000 & 1,0000 & 1,0000\\
				PublicTransitSystem & 0,9744 & 0,7308 & 0,8352\\
				Ship & 1,0000 & 0,6000 & 0,7500\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 0,0000 & 1,0000 & 0,0000\\
				SportsTeam & 0,0000 & 1,0000 & 0,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9677} & \textbf{0,7965} & \textbf{0,8738}\\\hline
		\end{tabular}
	\end{table}		
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9956 & 0,9898 & 0,9927\\\hline
				\textbf{Totals} & \textbf{0,9956} & \textbf{0,9898} & \textbf{0,9927}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 1,0000 & 0,9878 & 0,9939\\
				PoliticalParty & 0,9950 & 0,9852 & 0,9901\\
				Politician & 0,9937 & 0,9906 & 0,9922\\\hline
				\textbf{Totals} & \textbf{0,9956} & \textbf{0,9883} & \textbf{0,9920}\\\hline
		\end{tabular}
	\end{table}	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,9963 & 0,9963 & 0,9963\\\hline
				\textbf{Totals} & \textbf{0,9963} & \textbf{0,9963} & \textbf{0,9963}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 0,9722 & 0,9859\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 1,0000 & 0,9878 & 0,9939\\
				SportsEvent & 1,0000 & 0,9767 & 0,9882\\
				SportsLeague & 1,0000 & 1,0000 & 1,0000\\
				SportsManager & 1,0000 & 1,0000 & 1,0000\\				
				SportsTeam & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9888} & \textbf{0,9944}\\\hline
		\end{tabular}
	\end{table}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 1,0000 & 0,9912 & 0,9956\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9912} & \textbf{0,9956}\\\hline
		\end{tabular}
	\end{table}
	
	
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\				
				Infrastructure & 1,0000 & 1,0000 & 1,0000\\
				PublicTransitSystem & 1,0000 & 0,9808 & 0,9903\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 0,6667 & 0,8000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{1,0000} & \textbf{0,9735} & \textbf{0,9865}\\\hline
		\end{tabular}
	\end{table}	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experiments that have more than 300 abstracts in model and test files}
%Top 400 links
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9804 & 0,9434 & 0,9615\\
				SPORT & 0,9832 & 0,9590 & 0,9709\\
				TRANSPORTATION & 0,9941 & 0,9754 & 0,9847\\\hline
				\textbf{Totals} & \textbf{0,9849} & \textbf{0,9584} & \textbf{0,9714}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9754 & 0,4082 & 0,5756\\
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9531} & \textbf{0,4082} & \textbf{0,5716}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,0000 & 1,0000 & 0,0000\\
				SPORT & 0,9837 & 0,9588 & 0,9711\\
				TRANSPORTATION & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9805} & \textbf{0,9588} & \textbf{0,9695}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,0000 & 1,0000 & 0,0000\\
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 0,9939 & 0,9762 & 0,9806\\\hline
				\textbf{Totals} & \textbf{0,9861} & \textbf{0,9822} & \textbf{0,9841}\\\hline
		\end{tabular}
	\end{table}	
		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Athlete & 1,0000 & 0,9899 & 0,9949\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\ 
				Coach & 1,0000 & 1,0000 & 1,0000\\
				Infrastructure & 1,0000 & 0,9896 & 0,9948\\
				PoliticalParty & 0,9766 & 0,9486 & 0,9624\\
				Politician & 1,0000 & 0,9893 & 0,9946\\
				PublicTransitSystem & 0,9935 & 0,9776 & 0,9855\\
				Ship & 1,0000 & 0,9231 & 0,9600\\
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\ 
				SportsClub & 0,9796 & 0,9658 & 0,9726\\
				SportsEvent & 1,0000 & 0,8636 & 0,9268\\
				SportsLeague & 0,9698 & 0,9835 & 0,9766\\
				SportsManager & 1,0000 & 0,9726 & 0,9861\\
				SportsTeam & 1,0000 & 0,9851 & 0,9925\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9870} & \textbf{0,9709} & \textbf{0,9789}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,0000 & 1,0000 & 0,0000\\				
				Election & 0,0000 & 0,0000 & 0,0000\\
				PoliticalParty & 0,9766 & 0,9484 & 0,9623\\
				Politician & 1,0000 & 0,2092 & 0,3460\\
				PublicTransitSystem & 0,0000 & 1,0000 & 0,0000\\
				Ship & 0,0000 & 1,0000 & 0,0000\\
				SportsClub & 0,0000 & 1,0000 & 0,0000\\
				SportsLeague & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9619} & \textbf{0,4151} & \textbf{0,5799}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,0000 & 1,0000 & 0,0000\\
				Athlete & 1,0000 & 0,9899 & 0,9949\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				PoliticalParty & 0,0000 & 1,0000 & 0,0000\\
				Politician & 0,0000 & 1,0000 & 0,0000\\
				SportsClub & 0,9794 & 0,9654 & 0,9724\\
				SportsEvent & 1,0000 & 0,8636 & 0,9268\\
				SportsLeague & 0,9696 & 0,9834 & 0,9765\\
				SportsManager & 1,0000 & 0,9726 & 0,9861\\				
				SportsTeam & 1,0000 & 0,9850 & 0,9924\\
				Train & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9821} & \textbf{0,9721} & \textbf{0,9770}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 1,0000 & 1,0000\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\				
				Infrastructure & 1,0000 & 0,9896 & 0,9948\\
				PoliticalParty & 0,0000 & 1,0000 & 0,0000\\				
				Politician & 0,0000 & 1,0000 & 0,0000\\				
				PublicTransitSystem & 0,9934 & 0,9773 & 0,9853\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 0,0000 & 1,0000 & 0,0000\\
				SportsTeam & 0,0000 & 1,0000 & 0,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9866} & \textbf{0,9866} & \textbf{0,9866}\\\hline
		\end{tabular}
	\end{table}		
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9866 & 0,9479 & 0,9669\\\hline
				\textbf{Totals} & \textbf{0,9866} & \textbf{0,9479} & \textbf{0,9669}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,9975 & 0,9590 & 0,9779\\
				PoliticalParty & 0,9767 & 0,9530 & 0,9647\\
				Politician & 0,9977 & 0,9920 & 0,9948\\\hline
				\textbf{Totals} & \textbf{0,9906} & \textbf{0,9717} & \textbf{0,9810}\\\hline
		\end{tabular}
	\end{table}	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,9858 & 0,9676 & 0,9766\\\hline
				\textbf{Totals} & \textbf{0,9858} & \textbf{0,8676} & \textbf{0,9766}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 0,9731 & 0,9863\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 0,9815 & 0,9715 & 0,9765\\
				SportsEvent & 1,0000 & 0,9091 & 0,9524\\
				SportsLeague & 0,9718 & 0,9787 & 0,9752\\
				SportsManager & 1,0000 & 0,9726 & 0,9850\\				
				SportsTeam & 1,0000 & 0,9900 & 0,9796\\\hline
				\textbf{Totals} & \textbf{0,9865} & \textbf{0,9727} & \textbf{0,9796}\\\hline
		\end{tabular}
	\end{table}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 0,9954 & 0,9747 & 0,9850\\\hline
				\textbf{Totals} & \textbf{0,9954} & \textbf{0,9747} & \textbf{0,9850}\\\hline
		\end{tabular}
	\end{table}
	
	
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,9835 & 0,9917\\
				Automobile & 1,0000 & 0,9583 & 0,9787\\				
				Infrastructure & 1,0000 & 0,9948 & 0,9974\\
				PublicTransitSystem & 0,9934 & 0,9773 & 0,9853\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 0,6667 & 0,8000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9970} & \textbf{0,9807} & \textbf{0,9888}\\\hline
		\end{tabular}
	\end{table}	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Top 500 Links}


	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9788 & 0,9444 & 0,9613\\
				SPORT & 0,9850 & 0,9596 & 0,9721\\
				TRANSPORTATION & 0,9962 & 0,9750 & 0,9855\\\hline
				\textbf{Totals} & \textbf{0,9857} & \textbf{0,9587} & \textbf{0,9720}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9734 & 0,4095 & 0,5765\\
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9549} & \textbf{0,4095} & \textbf{0,5732}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,0000 & 1,0000 & 0,0000\\
				SPORT & 0,9849 & 0,9594 & 0,9720\\
				TRANSPORTATION & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9788} & \textbf{0,9594} & \textbf{0,9690}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,0000 & 1,0000 & 0,0000\\
				SPORT & 0,0000 & 1,0000 & 0,0000\\
				TRANSPORTATION & 0,9961 & 0,9769 & 0,9864\\\hline
				\textbf{Totals} & \textbf{0,9870} & \textbf{0,9769} & \textbf{0,9819}\\\hline
		\end{tabular}
	\end{table}	
		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,9929 & 0,9964\\
				Athlete & 1,0000 & 0,9896 & 0,9948\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\ 
				Coach & 1,0000 & 1,0000 & 1,0000\\
				Infrastructure & 1,0000 & 0,9783 & 0,9890\\
				PoliticalParty & 0,9775 & 0,9403 & 0,9585\\
				Politician & 1,0000 & 0,9874 & 0,9937\\
				PublicTransitSystem & 0,9944 & 0,9807 & 0,9875\\
				Ship & 1,0000 & 0,9259 & 0,9615\\
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\ 
				SportsClub & 0,9756 & 0,9553 & 0,9654\\
				SportsEvent & 1,0000 & 0,8796 & 0,9360\\
				SportsLeague & 0,9700 & 0,9810 & 0,9755\\
				SportsManager & 1,0000 & 0,9780 & 0,9889\\
				SportsTeam & 1,0000 & 0,9831 & 0,9915\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9866} & \textbf{0,9669} & \textbf{0,9766}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,0000 & 1,0000 & 0,0000\\				
				Election & 0,0000 & 0,0000 & 0,0000\\
				PoliticalParty & 0,9774 & 0,9400 & 0,9583\\
				Politician & 1,0000 & 0,2171 & 0,3567\\
				PublicTransitSystem & 0,0000 & 1,0000 & 0,0000\\
				Ship & 0,0000 & 1,0000 & 0,0000\\
				SportsClub & 0,0000 & 1,0000 & 0,0000\\
				SportsLeague & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9631} & \textbf{0,4138} & \textbf{0,5789}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,0000 & 1,0000 & 0,0000\\
				Athlete & 1,0000 & 0,9896 & 0,9948\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				PoliticalParty & 0,0000 & 1,0000 & 0,0000\\
				Politician & 0,0000 & 1,0000 & 0,0000\\
				SportsClub & 0,9753 & 0,9549 & 0,9650\\
				SportsEvent & 1,0000 & 0,8796 & 0,9360\\
				SportsLeague & 0,9717 & 0,9810 & 0,9763\\
				SportsManager & 1,0000 & 0,9780 & 0,9889\\				
				SportsTeam & 1,0000 & 0,9831 & 0,9915\\
				Train & 0,0000 & 1,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,9785} & \textbf{0,9690} & \textbf{0,9737}\\\hline
		\end{tabular}
	\end{table}	

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,9927 & 0,9963\\
				Automobile & 1,0000 & 1,0000 & 1,0000\\				
				Infrastructure & 1,0000 & 0,9783 & 0,9890\\
				PoliticalParty & 0,0000 & 1,0000 & 0,0000\\				
				Politician & 0,0000 & 1,0000 & 0,0000\\				
				PublicTransitSystem & 0,9943 & 0,9804 & 0,9873\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 1,0000 & 1,0000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 0,0000 & 1,0000 & 0,0000\\
				SportsTeam & 0,0000 & 1,0000 & 0,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9884} & \textbf{0,9833} & \textbf{0,9858}\\\hline
		\end{tabular}
	\end{table}		
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				POLITICS & 0,9808 & 0,9450 & 0,9626\\\hline
				\textbf{Totals} & \textbf{0,9808} & \textbf{0,9450} & \textbf{0,9626}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Election & 0,9915 & 0,9393 & 0,9647\\
				PoliticalParty & 0,9777 & 0,9502 & 0,9637\\
				Politician & 0,9962 & 0,9877 & 0,9919\\\hline
				\textbf{Totals} & \textbf{0,9890} & \textbf{0,9648} & \textbf{0,9768}\\\hline
		\end{tabular}
	\end{table}	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				SPORT & 0,9856 & 0,9706 & 0,9780\\\hline
				\textbf{Totals} & \textbf{0,9856} & \textbf{0,9706} & \textbf{0,9780}\\\hline
		\end{tabular}
	\end{table}

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Athlete & 1,0000 & 0,9791 & 0,9894\\
				Coach & 1,0000 & 1,0000 & 1,0000\\
				SportsClub & 0,9771 & 0,9630 & 0,9700\\
				SportsEvent & 1,0000 & 0,9074 & 0,9515\\
				SportsLeague & 0,9755 & 0,9848 & 0,9801\\
				SportsManager & 1,0000 & 0,9780 & 0,9889\\				
				SportsTeam & 1,0000 & 0,9915 & 0,9957\\\hline
				\textbf{Totals} & \textbf{0,9861} & \textbf{0,9731} & \textbf{0,9796}\\\hline
		\end{tabular}
	\end{table}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				TRANSPORTATION & 0,9974 & 0,9756 & 0,9864\\\hline
				\textbf{Totals} & \textbf{0,9974} & \textbf{0,9756} & \textbf{0,9864}\\\hline
		\end{tabular}
	\end{table}
	
	
	\begin{table}[H]\centering
		\caption{TABLE}
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 1,0000 & 0,9781 & 0,9889\\
				Automobile & 1,0000 & 0,8800 & 0,9362\\				
				Infrastructure & 1,0000 & 0,9870 & 0,9934\\
				PublicTransitSystem & 0,9915 & 0,9804 & 0,9860\\
				Ship & 1,0000 & 1,0000 & 1,0000\\				
				SpaceShuttle & 1,0000 & 0,6667 & 0,8000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\
				Train & 1,0000 & 1,0000 & 1,0000\\\hline
				\textbf{Totals} & \textbf{0,9961} & \textbf{0,9769} & \textbf{0,9864}\\\hline
		\end{tabular}
	\end{table}	
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
\subsection{MIXED}
\begin{table}[H]\centering
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,9242 & 0,5755 & 0,7093\\
				Athlete & 0,8182 & 0,3778 & 0,5169\\
				Automobile & 0,9565 & 0,3607 & 0,5238\\ 
				Coach & 1,0000 & 0,2000 & 0,3333\\
				Infrastructure & 1,0000 & 0,9896 & 0,9948\\
				Locomotive & 0,0000 & 0,0000 & 0,0000\\
				Motorcycle & 0,0000 & 0,0000 & 0,0000\\
				OrganisationMember & 0,0000 & 0,0000 & 0,0000\\				
				PoliticalParty & 0,7656 & 0,5819 & 0,6613\\
				Politician & 0,8925 & 0,4099 & 0,5618\\
				PublicTransitSystem & 0,8291 & 0,6178 & 0,7080\\
				Ship & 0,9375 & 0,3409 & 0,5000\\
				SpaceShuttle & 1,0000 & 0,3750 & 0,5455\\
				SpaceStation & 1,0000 & 0,3333 & 0,5000\\ 
				SportsClub & 0,8009 & 0,4276 & 0,5575\\
				SportsEvent & 0,9559 & 0,3171 & 0,4762\\
				SportsLeague & 0,8071 & 0,5912 & 0,6824\\
				SportsManager & 0,9643 & 0,2903 & 0,4463\\
				SportsTeam & 0,8856 & 0,5838 & 0,7037\\
				Train & 1,0000 & 0,5455 & 0,7059\\\hline
				\textbf{Totals} & \textbf{0,8206} & \textbf{0,4837} & \textbf{0,6087}\\\hline
		\end{tabular}
		\caption{All 3 Domains Fine Grained Top 300 With All 3 Domains Fine Grained Top 500 Links And All 3 Domains Fine Grained Top 500 Links With Lower PageRank}
	\end{table}


\begin{table}[H]\centering
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,9735 & 0,6934 & 0,8099\\
				Athlete & 0,9101 & 0,6222 & 0,7391\\
				Automobile & 1,0000 & 0,4098 & 0,5814\\ 
				Coach & 1,0000 & 0,3000 & 0,4615\\
				Infrastructure & 0,8885 & 0,5218 & 0,6575\\
				Locomotive & 0,0000 & 0,0000 & 0,0000\\
				Motorcycle & 0,0000 & 0,0000 & 0,0000\\
				OrganisationMember & 0,0000 & 0,0000 & 0,0000\\				
				PoliticalParty & 0,8393 & 0,7403 & 0,7876\\
				Politician & 0,9271 & 0,6593 & 0,7706\\
				PublicTransitSystem & 0,9027 & 0,7389 & 0,8126\\
				Rocket & 0,0000 & 0,0000 & 0,0000\\				
				Ship & 0,9615 & 0,5682 & 0,7143\\
				SpaceShuttle & 1,0000 & 0,4375 & 0,6087\\
				SpaceStation & 1,0000 & 0,6667 & 0,8000\\ 
				SportsClub & 0,8722 & 0,6071 & 0,7159\\
				SportsEvent & 0,9000 & 0,4829 & 0,6286\\
				SportsLeague & 0,8622 & 0,7357 & 0,7939\\
				SportsManager & 0,9787 & 0,4946 & 0,6571\\
				SportsTeam & 0,9276 & 0,7514 & 0,8302\\
				Train & 1,0000 & 0,5455 & 0,7059\\\hline
				\textbf{Totals} & \textbf{0,8844} & \textbf{0,6592} & \textbf{0,7553}\\\hline
		\end{tabular}
		\caption{All 3 Domains Fine Grained Top 500 Links  With All 3 Domains Fine Grained Top 500 Links And All 3 Domains Fine Grained Top 500 Links With Lower Page Rank}
	\end{table}


\begin{table}[H]\centering
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,6667 & 0,1111 & 0,1905\\
				Athlete & 0,4675 & 0,1268 & 0,1994\\
				Automobile & 0,0000 & 0,0000 & 0,0000\\ 
				Coach & 0,0000 & 0,0000 & 0,0000\\
				Infrastructure & 0,5352 & 0,1387 & 0,2203\\
				Locomotive & 0,0000 & 0,0000 & 0,0000\\
				Motorcycle & 0,0000 & 0,0000 & 0,0000\\
				OrganisationMember & 0,0000 & 0,0000 & 0,0000\\				
				PoliticalParty & 0,5462 & 0,4097 & 0,4682\\
				Politician & 0,5962 & 0,1867 & 0,2844\\
				PublicTransitSystem & 0,6943 & 0,4098 & 0,5154\\
				Rocket & 0,0000 & 0,0000 & 0,0000\\				
				Ship & 0,0000 & 0,0000 & 0,0000\\
				SpaceShuttle & 0,0000 & 0,0000 & 0,0000\\
				SpaceStation & 0,0000 & 0,0000 & 0,0000\\ 
				SportsClub & 0,6370 & 0,2675 & 0,3768\\
				SportsEvent & 0,2667 & 0,0412 & 0,0714\\
				SportsLeague & 0,6395 & 0,4125 & 0,5015\\
				SportsManager & 0,6000 & 0,0316 & 0,0600\\
				SportsTeam & 0,6316 & 0,2975 & 0,4045\\
				Train & 0,0000 & 0,0000 & 0,0000\\\hline
				\textbf{Totals} & \textbf{0,5983} & \textbf{0,2670} & \textbf{0,3692}\\\hline
		\end{tabular}
		\caption{All 3 Domains Fine Grained Top 500 Links With All 3 Domains Fine Grained Top 500 Links With Lower PageRank}
	\end{table}

\begin{table}[H]\centering
		\label{}
		\begin{tabular}{|l|l|l|l|}
			\hline {\textbf{Entity}} & {\textbf{P}} & {\textbf{R}} & {\textbf{F1}}\\\hline
				Aircraft & 0,9950 & 0,9706 & 0,9826\\
				Athlete & 0,0000 & 0,0000 & 0,0000\\
				Automobile & 1,0000 & 0,8500 & 0,9189\\ 
				Coach & 0,0000 & 0,0000 & 0,0000\\
				Infrastructure & 1,0000 & 0,9820 & 0,9909\\
				PoliticalParty & 0,0000 & 0,0000 & 0,0000\\
				Politician & 0,0000 & 0,0000 & 0,0000\\
				PublicTransitSystem & 0,9835 & 0,9676 & 0,9755\\
				Ship & 1,0000 & 0,9655 & 0,9825\\
				SpaceShuttle & 1,0000 & 0,6667 & 0,8000\\
				SpaceStation & 1,0000 & 1,0000 & 1,0000\\ 
				SportsClub & 0,0000 & 0,0000 & 0,0000\\
				SportsEvent & 0,0000 & 0,0000 & 0,0000\\
				SportsLeague & 0,0000 & 0,0000 & 0,0000\\
				SportsManager & 0,0000 & 0,0000 & 0,0000\\
				SportsTeam & 0,0000 & 0,0000 & 0,0000\\
				Train & 1,0000 & 0,9091 & 0,9524\\\hline
				\textbf{Totals} & \textbf{0,9909} & \textbf{0,3491} & \textbf{0,5163}\\\hline
		\end{tabular}
		\caption{Transportation Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 300 Links And Transportation Fine Grained Top 300 Links}
	\end{table}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiments with lower training abstracts on model, but higher abstracts on test file}	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop10LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 10 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop10LinksRunnedWithAll3DomainsFineGrainedTop300Links".png}
		\caption{All 3 Domains Fine Grained Top 10 Links Runned With All 3 Domains Fine Grained Top 300 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop10LinksRunnedWithAll3DomainsFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 10 Links Runned With All 3 Domains Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop10LinksRunnedWithAll3DomainsFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 10 Links Runned With All 3 Domains Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithAll3DomainsFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With All 3 Domains Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithAll3DomainsFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With All 3 Domains Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithPoliticsFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With Politics Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithPoliticsFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With Politics Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithSportFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With Sport Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithSportFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With Sport Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithTransportationFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With Transportation Fine Grained Top 500 Links}\label{}
	\end{figure}

	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop100LinksRunnedWithTransportationFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 100 Links Runned With Transportation Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithAll3DomainsFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With All 3 Domains Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithAll3DomainsFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With All 3 Domains Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithPoliticsFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Politics Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithPoliticsFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Politics Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithSportFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Sport Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithSportFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Sport Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithTransportationFineGrainedTop500Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Transportation Fine Grained Top 500 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithLowerLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithTransportationFineGrainedTop500LinksWithLowerPageRank".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Transportation Fine Grained Top 500 Links With Lower PageRank}\label{}
	\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experiments with higher training abstracts on model, but lower abstracts on test file}	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithAll3DomainsFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With All 3 Domains Fine Grained Top 10 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithPoliticsFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Politics Fine Grained Top 10 Links}\label{}
	\end{figure}
		
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithPoliticsFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Politics Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithSportFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Sport Fine Grained Top 10 Links}\label{}
	\end{figure}
				
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithSportFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Sport Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithTransportationFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Transportation Fine Grained Top 10 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop300LinksRunnedWithTransportationFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 300 Links Runned With Transportation Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 10 Links}\label{}
	\end{figure}	
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}	
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop300Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 300 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithPoliticsFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Politics Fine Grained Top 10 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithPoliticsFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Politics Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithPoliticsFineGrainedTop300Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Politics Fine Grained Top 300 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithSportFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Sport Fine Grained Top 10 Links}\label{}
	\end{figure}	
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithSportFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Sport Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithSportFineGrainedTop300Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Sport Fine Grained Top 300 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithTransportationFineGrainedTop10Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Transportation Fine Grained Top 10 Links}\label{}
	\end{figure}	
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithTransportationFineGrainedTop100Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Transportation Fine Grained Top 100 Links}\label{}
	\end{figure}	
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/All3DomainsFineGrainedTop500LinksRunnedWithTransportationFineGrainedTop300Links".png}
		\caption{All 3 Domains Fine Grained Top 500 Links Runned With Transportation Fine Grained Top 300 Links}\label{}
	\end{figure}	
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/PoliticsFineGrainedTop300LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{Politics Fine Grained Top 300 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}	
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/PoliticsFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{Politics Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/PoliticsFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop300Links".png}
		\caption{Politics Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 300 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/SportFineGrainedTop300LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{Sport Fine Grained Top 300 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/SportFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{Sport Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}
	
	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/SportFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop300Links".png}
		\caption{Sport Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 300 Links}\label{}
	\end{figure}


	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/TransportationFineGrainedTop300LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{Transportation Fine Grained Top 300 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}	

	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/TransportationFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop100Links".png}
		\caption{Transportation Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 100 Links}\label{}
	\end{figure}	

	\begin{figure}[H]\centering
		\includegraphics[width=\textwidth]{"/Mixed/WithHigherLinksOnModels/TransportationFineGrainedTop500LinksRunnedWithAll3DomainsFineGrainedTop300Links".png}
		\caption{Transportation Fine Grained Top 500 Links Runned With All 3 Domains Fine Grained Top 300 Links}\label{}
	\end{figure}			
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
\section{Summary of results}
\subsection{Graphs}
%	\begin{figure}[H]\centering
%		\includegraphics[width=\textwidth]{"C:/Users/Jakovcheski/Desktop/Experiments Graphs/Top500Links/Top500LinksF1".png}
%		\caption{Top 500 Links F1}\label{}
%	\end{figure}
%	
%		\begin{figure}[H]\centering
%		\includegraphics[width=\textwidth]{"C:/Users/Jakovcheski/Desktop/Experiments Graphs/Total/TotalAll3DomainsCoarseGrainedRunnedWithAll3DomainsCoarseGrained-tsvF1".png}
%		\caption{All 3 Domains Coarse Grained Runned With All 3 Domains Coarse Grained-tsv F1}\label{}
%	\end{figure}



\begin{conclusion}
%	The word-based dictionary data compression algorithms (a part of lossless data compression) are the subject of this thesis. The lossless data compression is a very important field of research because the data compression allows to reduce the amount of space needed to store data.
%
%	The background of data compression field was presented in Chapter~\ref{textcompr}. There are basic notions and definitions followed by description of character-based dictionary algorithms. The word-based dictionary compression methods were investigated and discussed at the end of this chapter too.
%
%% 	There is the investigation of index distribution of tested files in Section \ref{sec:distr}. It led to the new modification of semi-adaptive word-based \gls{LZW} algorithm---\textit{WLZWE2}. The compression efficiency of this algorithm applied to the large files is better than the other implemented algorithms. However, the compression efficiency of \textit{WLZWE2} algorithm is much worse when it is applied to the small files. The experiments with \textit{WLZWE2} and \textit{WLZWES2} algorithms confirm the assumption from Section \ref{sec:distr}---the compression efficiency of version with unsorted dictionaries (\textit{WLZWE2}) is analogous to version with sorted ones (\textit{WLZWES2}).
%
%	The testing of memory used during compression and/or decompression process is one of the possibilities of further research. The experiments with files of greater size or multilingual files could be also good opportunity to gain new improvements of algorithms. The static part of dictionaries could improve the compression efficiency too.
%
%	The implemented methods achieve fairly good compression ratio (25--30$\%$ at large files) with acceptable compression and decompression time. There are possibilities of further improvements especially at semi-adaptive methods. However, the gain of these improvements is not good enough to top the compression efficiency of other lossless data compression methods (context methods from PPM family). The results of implemented algorithms were not as good as it was expected but the work on this thesis showed new ways of possible further research---word-based version of grammar-based compression algorithms and another possibilities in the field of word-based context methods of data compression.
%
%	The Gnuplot 4.2 utility was very useful for generation of graphs in this thesis. There was the drawing editor Ipe 6.0 used for figures creation.
%	\nocite{Po01}
\end{conclusion}

\bibliographystyle{iso690}
\bibliography{ref}


\appendix
\chapter{Retrieved types}
\section{POLITICS types}\label{PoliticsTypes}
Parliament, Election,PoliticalParty, GeopoliticalOrganisation, Politician, Ambassador, Chancellor, Congressman, Deputy, Governor, Lieutenant, Mayor, MemberOfParliament, Minister, President, PrimeMinister, Senator, VicePresident, VicePrimeMinister, PoliticianSpouse, PersonFunction, PoliticalFunction, Profession, TopicalConcept and PoliticalConcept.

\section{SPORT types}\label{SportTypes}
Types: Sport, firstOlympicEvent, footedness, TeamSport, SportsClub, HockeyClub, RugbyClub, SoccerClub, chairmanTitle, clubsRecordGoalscorer, fansgroup, firstGame, ground, largestWin, managerTitle, worstDefeat and NationalSoccerClub are grouped at SportsClub type.

Types: SportsLeague, AmericanFootballLeague, AustralianFootballLeague, AutoRacingLeague, BaseballLeague, BasketballLeague, BowlingLeague, BoxingLeague, CanadianFootballLeague, CricketLeague, CurlingLeague, CyclingLeague, FieldHockeyLeague, FormulaOneRacing, GolfLeague, HandballLeague, IceHockeyLeague, InlineHockeyLeague, LacrosseLeague, MixedMartialArtsLeague, MotorcycleRacingLeague, PaintballLeague, PoloLeague, RadioControlledRacingLeague, RugbyLeague, SoccerLeague, SoftballLeague, SpeedwayLeague, TennisLeague, VideogamesLeague and VolleyballLeague are grouped at SportsLeague type.

Types: SportsTeam, AmericanFootballTeam, AustralianFootballTeam, BaseballTeam, BasketballTeam, CanadianFootballTeam, CricketTeam, CyclingTeam, FormulaOneTeam, HandballTeam, HockeyTeam and SpeedwayTeam are grouped at SportsTeam type.

Types: Athlete, ArcherPlayer, AthleticsPlayer, AustralianRulesFootballPlayer, BadmintonPlayer, BaseballPlayer, BasketballPlayer, Bodybuilder, Boxer, AmateurBoxer, BullFighter, Canoeist, ChessPlayer, Cricketer, Cyclist, DartsPlayer, Fencer, GaelicGamesPlayer, GolfPlayer, GridironFootballPlayer, AmericanFootballPlayer, CanadianFootballPlayer, Gymnast, HandballPlayer, HighDiver, HorseRider, Jockey, LacrossePlayer, MartialArtist, MotorsportRacer, MotorcycleRider, MotocycleRacer, SpeedwayRider, RacingDriver, DTMRacer, FormulaOneRacer, NascarDriver, RallyDriver, NationalCollegiateAthleticAssociationAthlete, NetballPlayer, PokerPlayer, Rower, RugbyPlayer, SnookerPlayer, SnookerChamp, SoccerPlayer, SquashPlayer, Surfer, Swimmer, TableTennisPlayer, TeamMember, TennisPlayer, VolleyballPlayer, BeachVolleyballPlayer , WaterPoloPlayer, WinterSportPlayer, Biathlete, BobsleighAthlete , CrossCountrySkier, Curler, FigureSkater, IceHockeyPlayer, NordicCombined, Skater, Ski\_jumper, Skier, SpeedSkater, Wrestler, SumoWrestler, Athletics and currentWorldChampion are grouped at Athlete type.

Types: Coach, AmericanFootballCoach, CollegeCoach and VolleyballCoach are grouped at Coach type.

Types: OrganizationMember, SportsTeamMember are grouped at OrganizationMember type.

Types: SportsManager, SoccerManager are grouped at SportsManager type.

Types: SportsEvent, CyclingCompetition, FootballMatch, GrandPrix, InternationalFootballLeagueEvent, MixedMartialArtsEvent, NationalFootballLeagueEvent, Olympics, OlympicEvent, Race, CyclingRace, HorseRace, MotorRace, Tournament, GolfTournament, SoccerTournament, TennisTournament, WomensTennisAssociationTournament, WrestlingEvent, SportCompetitionResult, OlympicResult, SnookerWorldRanking, SportsSeason, MotorsportSeason, SportsTeamSeason, BaseballSeason, FootballLeagueSeason, NationalFootballLeagueSeason, NCAATeamSeason, SoccerClubSeason, SoccerLeagueSeason and MotorSportSeason are grouped at SportsEvent type. 


%Sport, Athletics, currentWorldChampion, firstOlympicEventfootedness, TeamSport, SportsClub, HockeyClub, RugbyClub, SoccerClub, chairmanTitle, clubsRecordGoalscorer, fansgroup, firstGameground, largestWinmanagerTitle, worstDefeat, NationalSoccerClub, SportsLeague, AmericanFootballLeague, AustralianFootballLeague, AutoRacingLeague, BaseballLeague, BasketballLeague, BowlingLeague, BoxingLeague, CanadianFootballLeague, CricketLeague, CurlingLeague, CyclingLeague, FieldHockeyLeague, FormulaOneRacing, GolfLeague, HandballLeague, IceHockeyLeague, InlineHockeyLeague, LacrosseLeague, MixedMartialArtsLeague, MotorcycleRacingLeague, PaintballLeague, PoloLeague, RadioControlledRacingLeague, RugbyLeague, SoccerLeague, SoftballLeague, SpeedwayLeague, TennisLeague, VideogamesLeague, VolleyballLeague, SportsTeam, AmericanFootballTeam, AustralianFootballTeam, BaseballTeam, BasketballTeam, CanadianFootballTeam, CricketTeam, CyclingTeam, FormulaOneTeam, HandballTeam, HockeyTeam, SpeedwayTeam, Athlete, ArcherPlayer, AthleticsPlayer, AustralianRulesFootballPlayer, BadmintonPlayer, BaseballPlayer, BasketballPlayer, Bodybuilder, Boxer, AmateurBoxer, BullFighter, Canoeist, ChessPlayer, Cricketer, Cyclist, DartsPlayer, Fencer, GaelicGamesPlayer, GolfPlayer, GridironFootballPlayer, AmericanFootballPlayer, CanadianFootballPlayer, Gymnast, HandballPlayer, HighDiver, HorseRider, Jockey, LacrossePlayer, MartialArtist, MotorsportRacer, MotorcycleRider, MotocycleRacer, SpeedwayRider, RacingDriver, DTMRacer, FormulaOneRacer, NascarDriver, RallyDriver, NationalCollegiateAthleticAssociationAthlete, NetballPlayer, PokerPlayer, Rower, RugbyPlayer, SnookerPlayer, SnookerChamp, SoccerPlayer, SquashPlayer, Surfer, Swimmer, TableTennisPlayer, TeamMember, TennisPlayer, VolleyballPlayer, BeachVolleyballPlayer, WaterPoloPlayer, WinterSportPlayer, Biathlete, BobsleighAthlete, CrossCountrySkier, Curler, FigureSkater, IceHockeyPlayer, NordicCombined, Skater, Ski jumper, Skier, SpeedSkater, Wrestler, SumoWrestler, Coach, AmericanFootballCoach, CollegeCoach, VolleyballCoach, OrganisationMember, SportsTeamMember, SportsManager, SoccerManager, SportsEvent, CyclingCompetition, FootballMatch, GrandPrix, InternationalFootballLeagueEvent, MixedMartialArtsEvent, NationalFootballLeagueEvent, Olympics, OlympicEvent, Race, CyclingRace, HorseRace, MotorRace, Tournament, GolfTournament, SoccerTournament, TennisTournament, WomensTennisAssociationTournament, WrestlingEvent, SportCompetitionResult, OlympicResult, SnookerWorldRanking, SportsSeason, MotorsportSeason, SportsTeamSeason, BaseballSeason, FootballLeagueSeason, NationalFootballLeagueSeason, NCAATeamSeason, SoccerClubSeason, SoccerLeagueSeason, Referee, SportFacility, CricketGround, GolfCourse, RaceTrack and SkiArea.

\section{TRANSPORTATION types}\label{TransportationTypes}
Types: Aircraft, aircraftType, aircraftUser, ceiling, dischargeAverage, enginePower, engineType, gun, powerType, wingArea, wingspan and MilitaryAircraft are grouped at Aircraft type.

Types: Automobile, automobilePlatform, bodyStyle, enginePower, engineType, powerType, transmission and AutomobileEngine are grouped at Automobile type.

Types: Locomotive, boiler, boilerPressure and cylinderCount are grouped at Locomotive type.

Types: MilitaryVehicle, Motorcycle and SpaceStation are not grouped, this are leaved as it is. 

Types: On-SiteTransportation, ConveyorSystem, Escalator and MovingWalkway are grouped at On-SiteTransportation type.

Types: Rocket, countryOrigin, finalFlight, lowerEarthOrbitPayload, maidenFlight, rocketFunction, rocketStages and RocketEngine are gruped at Rocket type.

Types: Ship, captureDate, homeport, layingDown, maidenVoyage, numberOfPassengers, shipCrew and shipLaunch are grouped at Ship type.

Types: SpaceShuttle, contractAward, Crews, firstFlight, lastFlight, missions, numberOfCrew, numberOfLaunches and satellitesDeployed  are grouped at SpaceShuttle type.
 


Types: Spacecraft, cargoFuel, cargoGas, cargoWater and rocket are grouped at Spacecraft type 

Types: Train, locomotive, wagon and TrainCarriage are grouped at Train type.

Types: Tram, PublicTransitSystem, Airline and BusCompany are gruped at PublicTransitSystem type.

Types: Infrastructure, Airport, Port, RestArea, RouteOfTransportation, Bridge, RailwayLine, RailwayTunnel, Road, RoadJunction, RoadTunnel, WaterwayTunnel, Station, MetroStation, RailwayStation, RouteStop and TramStation are grouped at Infrastructure type.

\chapter{Stanford NER properties file}\label{NERpropertiesFile}
Here is the example of one of the properties file that we use for creating model with all used flags:
\begin{lstlisting}
	# location of the training file
	trainFile = 
	# location where you would like to save 
	#(serialize) your
	# classifier; adding .gz at the end 
	#automatically gzips the file,
	# making it smaller, and faster to load
	serializeTo = 

	# structure of your training file; 
	# this tells the classifier that
	# the word is in column 0 and the
	# correct answer is in column 1
	map = word=0,answer=1

	# This specifies the order of the CRF:
	# order 1 means that features
	# apply at most to a class pair of 
	# previous class and current class
	# or current class and next class.
	maxLeft=1

	# these are the features we'd like to 
	# train with
	# some are discussed below, the rest can 
	# be understood by looking 
	# at NERFeatureFactory
	useClassFeature=true
	useWord=true
	# word character ngrams will be included 
	# up to length 6 as prefixes
	# and suffixes only 
	useNGrams=true
	noMidNGrams=true
	maxNGramLeng=6
	usePrev=true
	useNext=true
	useDisjunctive=true
	useSequences=true
	usePrevSequences=true
	saveFeatureIndexToDisk=true
	useObservedSequencesOnly=true
	# the last 4 properties deal with word 
	# shape features
	useTypeSeqs=true
	useTypeSeqs2=true
	useTypeySequences=true
	wordShape=chris2useLC
\end{lstlisting}



%\begin{longtable}[htb]\centering
%%		\label{}
%%		\begin{tabular}{|l|l|l|l|}
%			\hline {\textbf{POLITICS}} & {\textbf{SPORT}} & {\textbf{TRANSPORTATION}}\\\hline
%Parliament & AustralianFootballLeague & Aircraft\\
%Election &  AutoRacingLeague & aircraftType\\
%PoliticalParty & BaseballLeague & aircraftUser\\
%GeopoliticalOrganisation & BasketballLeague & ceiling\\
%Politician & BowlingLeague & dischargeAverage\\
%Ambassador & BoxingLeague & enginePower\\
%Chancellor & CanadianFootballLeague & engineType\\
%Congressman & CricketLeague & gun\\
%Deputy & CurlingLeague & powerType\\
%Governor & CyclingLeague & wingArea\\
%Lieutenant & FieldHockeyLeague & wingspan\\
%Mayor & FormulaOneRacing & MilitaryAircraft\\
%MemberOfParliament & GolfLeague & Automobile\\
%Minister & HandballLeague & automobilePlatform\\
%President & IceHockeyLeague & bodyStyle\\
%PrimeMinister & InlineHockeyLeague & transmission\\
%Senator & LacrosseLeague & Locomotive\\
%VicePresident & MixedMartialArtsLeague & boiler\\
%VicePrimeMinister & MotorcycleRacingLeague & boilerPressure\\
%PoliticianSpouse & PaintballLeague & cylinderCount\\
%PersonFunction & PoloLeague & MilitaryVehicle\\
%PoliticalFunction & RadioControlledRacingLeague & Motorcycle\\
%Profession & RugbyLeague & On-SiteTransportation\\
%TopicalConcept & SoccerLeague & ConveyorSystem\\
%PoliticalConcept & SoftballLeague & Escalator\\
%& SpeedwayLeague & MovingWalkway\\
%& TennisLeague & Rocket\\
%& VideogamesLeague & countryOrigin\\
%& VolleyballLeague & finalFlight\\
%& SportsTeam & lowerEarthOrbitPayload\\
%& AmericanFootballTeam & maidenFlight\\
%& AustralianFootballTeam & rocketFunction\\
%& BaseballTeam & rocketStages\\
%& BasketballTeam & Ship\\
%& CanadianFootballTeam & captureDate\\
%& CricketTeam & homeport\\
%& CyclingTeam & layingDown\\
%& FormulaOneTeam & maidenVoyage\\
%& HandballTeam & numberOfPassengers\\
%& HockeyTeam & shipCrew\\
%& SpeedwayTeam & shipLaunch\\
%& Athlete & SpaceShuttle\\
%& ArcherPlayer & contractAward\\
%& AthleticsPlayer & Crews\\
%& AustralianRulesFootballPlayer & firstFlight\\
%& BadmintonPlayer & lastFlight\\
%& BaseballPlayer & missions\\
%& BasketballPlayer & numberOfCrew\\
%& Bodybuilder & numberOfLaunchessatellitesDeployedSpaceStation\\
%& Boxer & Spacecraft\\
%& AmateurBoxer & cargoFuel\\
%& BullFighter & cargoGascargoWaterrocket\\
%& Canoeist & Train\\
%& ChessPlayer & locomotive\\
%& Cricketer & wagon\\
%& Cyclist & TrainCarriage\\
%& DartsPlayer & Tram\\
%& Fencer & Engine\\
%& GaelicGamesPlayer & AutomobileEngine\\
%& GolfPlayer & RocketEngine\\
%& GridironFootballPlayer & Company\\
%& AmericanFootballPlayer & PublicTransitSystem\\
%& CanadianFootballPlayer & Airline\\
%& Gymnast & BusCompany\\
%& HandballPlayer & Infrastructure\\
%& HighDiver & Airport\\
%& HorseRider & Port\\
%& Jockey & RestArea\\
%& LacrossePlayer & RouteOfTransportation\\
%& MartialArtist & Bridge\\
%& MotorsportRacer & RailwayLine\\
%& MotorcycleRider & RailwayTunnel\\
%& MotocycleRacer & Road\\
%& SpeedwayRider & RoadJunction\\
%& RacingDriver & RoadTunnel\\
%& DTMRacer & WaterwayTunnel\\
%& FormulaOneRacer & Station\\
%& NascarDriver & MetroStation\\
%& RallyDriver & RailwayStation\\
%& NationalCollegiateAthleticAssociationAthlete & RouteStop\\
%& NetballPlayer & TramStation\\
%& PokerPlayer & \\
%& Rower & \\
%& RugbyPlayer & \\
%& SnookerPlayer & \\
%& SnookerChamp & \\
%& SoccerPlayer & \\
%& SquashPlayer & \\
%& Surfer & \\
%& Swimmer & \\
%& TableTennisPlayer & \\
%& TeamMember & \\
%& TennisPlayer & \\
%& VolleyballPlayer & \\
%& BeachVolleyballPlayer & \\
%& WaterPoloPlayer & \\
%& WinterSportPlayer & \\
%& Biathlete & \\
%& BobsleighAthlete & \\
%& CrossCountrySkier & \\
%& Curler & \\
%& FigureSkater & \\
%& IceHockeyPlayer & \\
%& NordicCombined & \\
%& Skater & \\
%& Skijumper & \\
%& Skier & \\
%& SpeedSkater & \\
%& Wrestler & \\
%& SumoWrestler & \\
%& Coach & \\
%& AmericanFootballCoach & \\
%& CollegeCoach & \\
%& VolleyballCoach & \\
%& OrganisationMember & \\
%& SportsTeamMember & \\
%& SportsManager & \\
%& SoccerManager & \\
%& SportsEvent & \\
%& CyclingCompetition & \\
%& FootballMatch & \\
%& GrandPrix & \\
%& InternationalFootballLeagueEvent & \\
%& MixedMartialArtsEvent & \\
%& NationalFootballLeagueEvent & \\
%& Olympics & \\
%& OlympicEvent & \\
%& Race & \\
%& CyclingRace & \\
%& HorseRace & \\
%& MotorRace & \\
%& Tournament & \\
%& GolfTournament & \\
%& SoccerTournament & \\
%& TennisTournament & \\
%& WomensTennisAssociationTournament & \\
%& WrestlingEvent & \\
%& SportCompetitionResult & \\
%& OlympicResult & \\
%& SnookerWorldRanking & \\
%& SportsSeason & \\
%& MotorsportSeason & \\
%& SportsTeamSeason & \\
%& BaseballSeason & \\
%& FootballLeagueSeason & \\
%& NationalFootballLeagueSeason & \\
%& NCAATeamSeason & \\
%& SoccerClubSeason & \\
%& SoccerLeagueSeason & \\
%& Referee & \\
%& SportFacility & \\
%& CricketGround & \\
%& GolfCourse & \\
%& RaceTrack & \\
%& SkiArea & \\\hline
%		%\end{tabular}
%		\caption{Retrieved types}
%	\end{longtable}
% \printglossaries

\chapter{Contents of CD}\label{app:CDcontent}

%Visualise the contents of enclosed media. Use of \verb|dirtree| is recommended. Note that directories src and text with appropriate contents are mandatory.


%\begin{figure}
%	\dirtree{%
%		.1 readme.txt\DTcomment{the file with CD contents description}.
%		.1 data\DTcomment{the data files directory}.
%		.2 graphs\DTcomment{the directory of graphs of experiments}.
%		.3 *.eps\DTcomment{the B/W graphs}.
%		.3 *.png\DTcomment{the color graphs}.
%		.3 *.dat\DTcomment{the graphs data files}.
%		.1 exe\DTcomment{the directory with executable WBDCM program}.
%		.2 wbdcm\DTcomment{the WBDCM program executable (UNIX)}.
%		.2 wbdcm.exe\DTcomment{the WBDCM program executable (Windows)}.
%		.1 src\DTcomment{the directory of source codes}.
%		.2 wbdcm\DTcomment{the directory of WBDCM program}.
%		.3 Makefile\DTcomment{the makefile of WBDCM program (UNIX)}.
%		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
%		.3 figures\DTcomment{the thesis figures directory}.
%		.3 *.tex\DTcomment{the \LaTeX{} source code files of the thesis}.
%		.1 text\DTcomment{the thesis text directory}.
%		.2 thesis.pdf\DTcomment{the Diploma thesis in PDF format}.
%		.2 thesis.ps\DTcomment{the Diploma thesis in PS format}.
%	}
%\end{figure}


\end{document}
